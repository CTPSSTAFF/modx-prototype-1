{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "closing-fishing",
   "metadata": {},
   "source": [
    "# Transit Boardings Report - version 2\n",
    "This notebook generates two types of report:\n",
    "- Standard: A single or comparison report for the overall scenario(s)\n",
    "    - This includes sub-mode daily boardings and TOD totals (AM, MD, PM, NT) \n",
    "- Detailed: A detailed report for selected links.\n",
    "    - This includes Daily and TOD boardings for user specified route(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e193f909",
   "metadata": {},
   "source": [
    "### This is a work-in-progress and currently (9/29/2021) ONLY to be used by a qualified developer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from functools import reduce\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from plotly import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One or two scenarios?\n",
    "# \n",
    "# This variable ('scenarios') was called 'bases' in the first version of this notebook.\n",
    "#\n",
    "scenarios = {'Base Model':r'G:/Regional_Modeling/1A_Archives/LRTP_2018/2040 NB Scen 01_MoDXoutputs/'\n",
    "             #,'Comparative Model':r'G:/Regional_Modeling/1A_Archives/LRTP_2018/2016 Scen 00_08March2019_MoDXoutputs/'\n",
    "            } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5edba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference data: CSV file containing list of _ALL_ transit routes:\n",
    "all_transit_routes_csv_fn = \\\n",
    "r'G:\\Regional_Modeling\\1A_Archives\\LRTP_2018\\2016 Scen 00_08March2019_MoDXoutputs\\Databases\\Statewide_Routes_2018S.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3009c5c",
   "metadata": {},
   "source": [
    "#### User input required: supply name of CSV file with list of routes on which to report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34536d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two types of reports:\n",
    "# Standard (all routes in the input CSV file you suppy) or detailed (in-line list of routes.)\n",
    "\n",
    "# CSV file containing list of transit routes for which to generate this report:\n",
    "routes_csv_fn = r'G:/Data_Resources/DataStore/transit info.csv'\n",
    "\n",
    "# Read this CSV file into a pandas dataframe:\n",
    "routes_df = pd.read_csv(routes_csv_fn)\n",
    "\n",
    "# Specify list of routes for which to generate report.\n",
    "# By default this is all routes in the 'routefile' dataframe:\n",
    "route_list = routes_df['Route_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83a78b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1efca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-exclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import transit assignment result CSV files for a given scenario.\n",
    "# 1. Read all CSV files for each time period ('tod'), and caclculate the sums for each time period.\n",
    "#    Step 1 can be performed as a brute-force sum across all columns, since the number of rows in\n",
    "#    the CSVs (and thus the dataframes) for any given time period are all the same.\n",
    "#\n",
    "# 2. Calculate the daily total across all time periods.\n",
    "#    Step 2 requires a bit of subtelty, because the number of rows in the data frames produced in \n",
    "#    Step 1 is NOT necessarily the same. A brute-force apporach will not work, generally speaking.\n",
    "#    See comments in the code below for details.\n",
    "#\n",
    "# 3. Return a dict of the form:\n",
    "#    {'AM'    : dataframe with totals for the AM period ,\n",
    "#     'MD'    : datafrme with totals for the MD period,\n",
    "#     'PM'    : dataframe with totals for the PM period,\n",
    "#     'NT'    : dataframe with totals for the NT period,\n",
    "#     'daily' : dataframe with totals for the entire day\n",
    "#   }\n",
    "# \n",
    "def import_transit_assignment(scenario):\n",
    "    '''bring in data and combine into sum tables for daily and put into a dictionary'''\n",
    "    base = scenario + r'out/'\n",
    "    tods = [\"AM\", \"MD\", \"PM\", \"NT\"]\n",
    "    # At the end of execution of this function, the dictionary variable'TODsums' will contain all the TOD summed results:\n",
    "    # one key-value-pair for each 'tod' AND the 'daily' total as well.\n",
    "    \n",
    "    # The dict 'TODsums' is the return value of this function.\n",
    "    # The 'daily' element of this dict is added below - see comments.\n",
    "    TODsums = { 'AM' : None, 'MD' : None, 'PM' : None, 'NT' : None }\n",
    "\n",
    "    # Import CSV files and create sum tables for each TOD and for the day as a whole\n",
    "    for tod in tods:\n",
    "        # Get full paths to _all_ CSV files for the current t-o-d (a.k.a. 'time period')\n",
    "        x = tod + '/' \n",
    "        fq_csv_fns = glob.glob(os.path.join(base,x,r'*.csv'))\n",
    "        # 'tablist' : List of all the dataframes created from reading in the all the CSV files for the current t-o-d\n",
    "        tablist = []\n",
    "        for csv_file in fq_csv_fns:\n",
    "            # Read CSV file into dataframe, set indices, and append to 'tablist'\n",
    "            tablist.append(pd.read_csv(csv_file).set_index(['ROUTE','STOP']))\n",
    "        #\n",
    "        \n",
    "        # Filter dataframe to include rows where 'ROUTE' is one of those selected to report on\n",
    "        for t in range(len(tablist)):\n",
    "            tablist[t] = tablist[t][tablist[t].index.get_level_values('ROUTE').isin(route_list)]\n",
    "        #\n",
    "        \n",
    "        # Sum the tables for the current TOD\n",
    "        TODsums[tod] = reduce(lambda a, b: a.add(b, fill_value=0), tablist)\n",
    "    # end_for over all tod's\n",
    "    \n",
    "    # Compute the daily sums.\n",
    "    #\n",
    "    # Step 1: Join 'am' and 'md' dataframes\n",
    "    j1 = pd.merge(am_results, md_results, on=['ROUTE', 'STOP'], how='outer', suffixes=('_am', '_md'))\n",
    "    # Step 1.1 Replace NaN's with 0's\n",
    "    j1 = j1.fillna(0)\n",
    "\n",
    "    # Step 1.2 Compute the 'AM' + 'MD' sums\n",
    "    j1['DirectTransferOff'] = j1['DirectTransferOff_am'] + j1['DirectTransferOff_md']\n",
    "    j1['DirectTransferOn'] = j1['DirectTransferOn_am'] + j1['DirectTransferOn_md']\n",
    "    j1['DriveAccessOn'] = j1['DriveAccessOn_am'] + j1['DriveAccessOn_md']\n",
    "    j1['EgressOff'] = j1['EgressOff_am'] + j1['EgressOff_md']\n",
    "    j1['Off'] = j1['Off_am'] + j1['Off_md']\n",
    "    j1['On'] = j1['On_am'] + j1['On_md']\n",
    "    j1['WalkAccessOn'] = j1['WalkAccessOn_am'] + j1['WalkAccessOn_md'] \n",
    "    j1['WalkTransferOff'] = j1['WalkTransferOff_am'] + j1['WalkTransferOff_md']\n",
    "    j1['WalkTransferOn'] = j1['WalkTransferOn_am'] + j1['WalkTransferOn_md']\n",
    "\n",
    "    # Step 1.3: Drop un-needed columns\n",
    "    cols_to_drop = ['DirectTransferOff_am', 'DirectTransferOff_md',\n",
    "                    'DirectTransferOn_am', 'DirectTransferOn_md',\n",
    "                    'DriveAccessOn_am', 'DriveAccessOn_md',\n",
    "                    'EgressOff_am','EgressOff_md',\n",
    "                    'Off_am', 'Off_md',\n",
    "                    'On_am', 'On_md',\n",
    "                    'WalkAccessOn_am', 'WalkAccessOn_md',\n",
    "                    'WalkTransferOff_am', 'WalkTransferOff_md',\n",
    "                    'WalkTransferOn_am', 'WalkTransferOn_md'\n",
    "                    ]\n",
    "    j1 = j1.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Step 2: j2 - join 'pm' and 'nt' data frames\n",
    "    j2 = pd.merge(pm_results, nt_results, on=['ROUTE', 'STOP'], how='outer', suffixes=('_pm', '_nt'))\n",
    "    # Step 2.1: Replace NaN's with 0's\n",
    "    j2 = j2.fillna(0)\n",
    "\n",
    "    # Step 2.2: Compute the 'PM' + 'NT' sums\n",
    "    j2['DirectTransferOff'] = j2['DirectTransferOff_pm'] + j2['DirectTransferOff_nt']\n",
    "    j2['DirectTransferOn'] = j2['DirectTransferOn_pm'] + j2['DirectTransferOn_nt']\n",
    "    j2['DriveAccessOn'] = j2['DriveAccessOn_pm'] + j2['DriveAccessOn_nt']\n",
    "    j2['EgressOff'] = j2['EgressOff_pm'] + j2['EgressOff_nt']\n",
    "    j2['Off'] = j2['Off_pm'] + j2['Off_nt']\n",
    "    j2['On'] = j2['On_pm'] + j2['On_nt']\n",
    "    j2['WalkAccessOn'] = j2['WalkAccessOn_pm'] + j2['WalkAccessOn_nt'] \n",
    "    j2['WalkTransferOff'] = j2['WalkTransferOff_pm'] + j2['WalkTransferOff_nt']\n",
    "    j2['WalkTransferOn'] = j2['WalkTransferOn_pm'] + j2['WalkTransferOn_nt']\n",
    "\n",
    "    # Step 2.3: Drop un-needed columns\n",
    "    cols_to_drop = ['DirectTransferOff_pm', 'DirectTransferOff_nt',\n",
    "                    'DirectTransferOn_pm', 'DirectTransferOn_nt',\n",
    "                    'DriveAccessOn_pm', 'DriveAccessOn_nt',\n",
    "                    'EgressOff_pm','EgressOff_nt',\n",
    "                    'Off_pm', 'Off_nt',\n",
    "                    'On_pm', 'On_nt',\n",
    "                    'WalkAccessOn_pm', 'WalkAccessOn_nt',\n",
    "                    'WalkTransferOff_pm', 'WalkTransferOff_nt',\n",
    "                    'WalkTransferOn_pm', 'WalkTransferOn_nt'\n",
    "                    ]\n",
    "    j2 = j2.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Step 3: Join \"j1\" and \"j2\" to produce a dataframe with the daily totals\n",
    "    daily_df = pd.merge(j1, j2, on=['ROUTE', 'STOP'], how='outer', suffixes=('_j1', '_j2'))\n",
    "    # Step 3.1 : Replace any NaN's with 0's. This line _shouldn't_ be needed - just being extra cautious.\n",
    "    daily_df = daily_df.fillna(0)\n",
    "\n",
    "    # Step 3.2 : Compute THE daily sums\n",
    "    daily_df['DirectTransferOff'] = daily_df['DirectTransferOff_j1'] + daily_df['DirectTransferOff_j2']\n",
    "    daily_df['DirectTransferOn'] = daily_df['DirectTransferOn_j1'] + daily_df['DirectTransferOn_j2']\n",
    "    daily_df['DriveAccessOn'] = daily_df['DriveAccessOn_j1'] + daily_df['DriveAccessOn_j2']\n",
    "    daily_df['EgressOff'] = daily_df['EgressOff_j1'] + daily_df['EgressOff_j2']\n",
    "    daily_df['Off'] = daily_df['Off_j1'] + daily_df['Off_j2']\n",
    "    daily_df['On'] = daily_df['On_j1'] + daily_df['On_j2']\n",
    "    daily_df['WalkAccessOn'] = daily_df['WalkAccessOn_j1'] + daily_df['WalkAccessOn_j2'] \n",
    "    daily_df['WalkTransferOff'] = daily_df['WalkTransferOff_j1'] + daily_df['WalkTransferOff_j2']\n",
    "    daily_df['WalkTransferOn'] = daily_df['WalkTransferOn_j1'] + daily_df['WalkTransferOn_j2']\n",
    "\n",
    "    # Step 3.3 : Drop un-needed columns\n",
    "    cols_to_drop = ['DirectTransferOff_j1', 'DirectTransferOff_j2',\n",
    "                    'DirectTransferOn_j1', 'DirectTransferOn_j2',\n",
    "                    'DriveAccessOn_j1', 'DriveAccessOn_j2',\n",
    "                    'EgressOff_j1','EgressOff_j2',\n",
    "                    'Off_j1', 'Off_j2',\n",
    "                    'On_j1', 'On_j2',\n",
    "                    'WalkAccessOn_j1', 'WalkAccessOn_j2',\n",
    "                    'WalkTransferOff_j1', 'WalkTransferOff_j2',\n",
    "                    'WalkTransferOn_j1', 'WalkTransferOn_j2'\n",
    "                    ]\n",
    "    daily_df = daily_df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Finally, we've got the 'daily' total dataframe!\n",
    "    TODsums['daily'] = daily_df\n",
    "\n",
    "    # Ensure that the ROUTE and STOP columns aren't indices.\n",
    "    for k in TODsums.keys():\n",
    "        TODsums[k] = TODsums[k].reset_index()\n",
    "    #\n",
    "    return TODsums\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e457fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86594a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = import_transit_assignment(scenarios['Base Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f24c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['daily']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c348c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1661dd90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36182737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************************************************************************************************\n",
    "# Old, prototyping code below this point.\n",
    "results = import_transit_assignment(scenarios['Base Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3723e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "am_results = results['AM']\n",
    "md_results = results['MD']\n",
    "pm_results = results['PM']\n",
    "nt_results = results['NT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b3bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start joining the tod-specific data frames\n",
    "# Join 'am' and 'md' dataframes\n",
    "j1 = pd.merge(am_results, md_results, on=['ROUTE', 'STOP'], how='outer', suffixes=('_am', '_md'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4962a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "j1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f847e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "j1 = j1.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51782d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "j1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c1575",
   "metadata": {},
   "outputs": [],
   "source": [
    "j1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623a44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "j1['DirectTransferOff'] = j1['DirectTransferOff_am'] + j1['DirectTransferOff_md']\n",
    "j1['DirectTransferOn'] = j1['DirectTransferOn_am'] + j1['DirectTransferOn_md']\n",
    "j1['DriveAccessOn'] = j1['DriveAccessOn_am'] + j1['DriveAccessOn_md']\n",
    "j1['EgressOff'] = j1['EgressOff_am'] + j1['EgressOff_md']\n",
    "j1['Off'] = j1['Off_am'] + j1['Off_md']\n",
    "j1['On'] = j1['On_am'] + j1['On_md']\n",
    "j1['WalkAccessOn'] = j1['WalkAccessOn_am'] + j1['WalkAccessOn_md'] \n",
    "j1['WalkTransferOff'] = j1['WalkTransferOff_am'] + j1['WalkTransferOff_md']\n",
    "j1['WalkTransferOn'] = j1['WalkTransferOn_am'] + j1['WalkTransferOn_md']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3011ca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "j1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de6f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "j1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b3dad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae3704d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['DirectTransferOff_am', 'DirectTransferOff_md',\n",
    "                'DirectTransferOn_am', 'DirectTransferOn_md',\n",
    "\t\t\t\t'DriveAccessOn_am', 'DriveAccessOn_md',\n",
    "\t\t\t\t'EgressOff_am','EgressOff_md',\n",
    "\t\t\t\t'Off_am', 'Off_md',\n",
    "\t\t\t\t'On_am', 'On_md',\n",
    "\t\t\t\t'WalkAccessOn_am', 'WalkAccessOn_md',\n",
    "\t\t\t\t'WalkTransferOff_am', 'WalkTransferOff_md',\n",
    "\t\t\t\t'WalkTransferOn_am', 'WalkTransferOn_md'\n",
    "\t\t\t\t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8857e535",
   "metadata": {},
   "outputs": [],
   "source": [
    "j1 = j1.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af7bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "j1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e376fb67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9a1c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94526d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# j2 - join 'pm' and 'nt' data frames\n",
    "j2 = pd.merge(pm_results, nt_results, on=['ROUTE', 'STOP'], how='outer', suffixes=('_pm', '_nt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5683d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "j2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a233dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "j2 = j2.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffddbc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "j2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a7bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "j2['DirectTransferOff'] = j2['DirectTransferOff_pm'] + j2['DirectTransferOff_nt']\n",
    "j2['DirectTransferOn'] = j2['DirectTransferOn_pm'] + j2['DirectTransferOn_nt']\n",
    "j2['DriveAccessOn'] = j2['DriveAccessOn_pm'] + j2['DriveAccessOn_nt']\n",
    "j2['EgressOff'] = j2['EgressOff_pm'] + j2['EgressOff_nt']\n",
    "j2['Off'] = j2['Off_pm'] + j2['Off_nt']\n",
    "j2['On'] = j2['On_pm'] + j2['On_nt']\n",
    "j2['WalkAccessOn'] = j2['WalkAccessOn_pm'] + j2['WalkAccessOn_nt'] \n",
    "j2['WalkTransferOff'] = j2['WalkTransferOff_pm'] + j2['WalkTransferOff_nt']\n",
    "j2['WalkTransferOn'] = j2['WalkTransferOn_pm'] + j2['WalkTransferOn_nt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff128eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "j2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cbcca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['DirectTransferOff_pm', 'DirectTransferOff_nt',\n",
    "                'DirectTransferOn_pm', 'DirectTransferOn_nt',\n",
    "\t\t\t\t'DriveAccessOn_pm', 'DriveAccessOn_nt',\n",
    "\t\t\t\t'EgressOff_pm','EgressOff_nt',\n",
    "\t\t\t\t'Off_pm', 'Off_nt',\n",
    "\t\t\t\t'On_pm', 'On_nt',\n",
    "\t\t\t\t'WalkAccessOn_pm', 'WalkAccessOn_nt',\n",
    "\t\t\t\t'WalkTransferOff_pm', 'WalkTransferOff_nt',\n",
    "\t\t\t\t'WalkTransferOn_pm', 'WalkTransferOn_nt'\n",
    "\t\t\t\t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1647a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "j2 = j2.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4606d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "j2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b98ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "j2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b7dfa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2a8273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48926d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join \"j1\" and \"j2\" to produce a dataframe with the daily total\n",
    "daily_df = pd.merge(j1, j2, on=['ROUTE', 'STOP'], how='outer', suffixes=('_j1', '_j2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a95022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following line _shouldn't_ be needed - just being cautious\n",
    "daily_df = daily_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d375e0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd010d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b9802",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df['DirectTransferOff'] = daily_df['DirectTransferOff_j1'] + daily_df['DirectTransferOff_j2']\n",
    "daily_df['DirectTransferOn'] = daily_df['DirectTransferOn_j1'] + daily_df['DirectTransferOn_j2']\n",
    "daily_df['DriveAccessOn'] = daily_df['DriveAccessOn_j1'] + daily_df['DriveAccessOn_j2']\n",
    "daily_df['EgressOff'] = daily_df['EgressOff_j1'] + daily_df['EgressOff_j2']\n",
    "daily_df['Off'] = daily_df['Off_j1'] + daily_df['Off_j2']\n",
    "daily_df['On'] = daily_df['On_j1'] + daily_df['On_j2']\n",
    "daily_df['WalkAccessOn'] = daily_df['WalkAccessOn_j1'] + daily_df['WalkAccessOn_j2'] \n",
    "daily_df['WalkTransferOff'] = daily_df['WalkTransferOff_j1'] + daily_df['WalkTransferOff_j2']\n",
    "daily_df['WalkTransferOn'] = daily_df['WalkTransferOn_j1'] + daily_df['WalkTransferOn_j2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76835bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f177b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b750bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b99482",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['DirectTransferOff_j1', 'DirectTransferOff_j2',\n",
    "                'DirectTransferOn_j1', 'DirectTransferOn_j2',\n",
    "\t\t\t\t'DriveAccessOn_j1', 'DriveAccessOn_j2',\n",
    "\t\t\t\t'EgressOff_j1','EgressOff_j2',\n",
    "\t\t\t\t'Off_j1', 'Off_j2',\n",
    "\t\t\t\t'On_j1', 'On_j2',\n",
    "\t\t\t\t'WalkAccessOn_j1', 'WalkAccessOn_j2',\n",
    "\t\t\t\t'WalkTransferOff_j1', 'WalkTransferOff_j2',\n",
    "\t\t\t\t'WalkTransferOn_j1', 'WalkTransferOn_j2'\n",
    "\t\t\t\t]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a0be1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71a6ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df = daily_df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a78e416",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a881e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['daily'] = daily_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f11c7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9bfa0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acefc1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8168931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data structure and function to map a TransCAD 'Mode' to the corresponding 'Meta-mode'\n",
    "_mode_to_metamode_mapping_table = {\n",
    "    1:  'MBTA_Bus',\n",
    "    2:  'MBTA_Bus',\n",
    "    3:  'MBTA_Bus' ,\n",
    "    4:  'Light_Rail',\n",
    "    5:  'Heavy_Rail',\n",
    "    6:  'Heavy_Rail',\n",
    "    7:  'Heavy_Rail',\n",
    "    8:  'Heavy_Rail',\n",
    "    9:  'Commuter_Rail',\n",
    "    10: 'Ferry',\n",
    "    11: 'Ferry',\n",
    "    12: 'Light_Rail',\n",
    "    13: 'Light_Rail',\n",
    "    14: 'Shuttle_Express',\n",
    "    15: 'Shuttle_Express',\n",
    "    16: 'Shuttle_Express',\n",
    "    17: 'RTA',\n",
    "    18: 'RTA',\n",
    "    19: 'RTA',\n",
    "    20: 'RTA',\n",
    "    21: 'RTA',\n",
    "    22: 'RTA',\n",
    "    23: 'Private',\n",
    "    24: 'Private',\n",
    "    25: 'Private',\n",
    "    26: 'Private',\n",
    "    27: 'Private',\n",
    "    28: 'Private',\n",
    "    29: 'Private',\n",
    "    30: 'Private',\n",
    "    31: 'Private',\n",
    "    32: 'Commuter_Rail',\n",
    "    33: 'Commuter_Rail',\n",
    "    34: 'Commuter_Rail',\n",
    "    35: 'Commuter_Rail',\n",
    "    36: 'Commuter_Rail',\n",
    "    37: 'Commuter_Rail',\n",
    "    38: 'Commuter_Rail',\n",
    "    39: 'Commuter_Rail',\n",
    "    40: 'Commuter_Rail',\n",
    "    41: 'Commuter_Rail',\n",
    "    42: 'Commuter_Rail',\n",
    "    43: 'Commuter_Rail',\n",
    "    44: 'Commuter_Rail',\n",
    "    70: 'Walk' }\n",
    "\n",
    "def mode_to_metamode(mode):\n",
    "    retval = 'None'\n",
    "    if mode in _mode_to_metamode_mapping_table:\n",
    "        return _mode_to_metamode_mapping_table[mode]\n",
    "    # end_if\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbb80f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_metamode_table(scenario):\n",
    "    '''flag each route type by metaMode'''\n",
    "    routemode = pd.read_csv(scenario + r'Databases/Statewide_Routes_2018S.csv', \n",
    "                            usecols=[\"Routes_ID\", \"Mode\"]).drop_duplicates()\n",
    "    routemode['metaMode'] = routemode.apply(lambda x: mode_to_metamode(x['Mode']), axis=1)\n",
    "    return routemode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d896aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c850e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b6666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_and_agg(TODSums, routemode):\n",
    "    '''aggregate the on and offs by route or metaMode'''\n",
    "#set the group by field depending on if standard or detailed report\n",
    "    if len(route_list) > 0:\n",
    "        agg = 'ROUTE'\n",
    "    else: \n",
    "        agg = 'metaMode'\n",
    "\n",
    "    for x in TODsums.keys():\n",
    "        if len(routeList)> 0:\n",
    "            TODsums[x] = routefile.merge(TODsums[x], how='outer', left_on='Route_ID', right_on='ROUTE')\n",
    "            TODsums[x]['ROUTE'] = TODsums[x]['Route_Name'].str.split('.:()').str[0]\n",
    "        #join each table to route mode\n",
    "            TODsums[x] = routemode.merge(TODsums[x], how='right', left_on='Routes_ID', right_on='Route_ID')\n",
    "        else:\n",
    "            TODsums[x] = routemode.merge(TODsums[x], how='right', left_on='Routes_ID', right_on='ROUTE')\n",
    "        #sum all On/Off fields by metamode \n",
    "        TODsums[x] = TODsums[x].groupby([agg])[['DirectTransferOff','DirectTransferOn','DriveAccessOn','EgressOff','Off','On',\n",
    "                                                'WalkAccessOn','WalkTransferOff','WalkTransferOn']].agg('sum').reset_index()\n",
    "    return TODsums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-leather",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots(scen2, g):\n",
    "    '''make graphs!'''\n",
    "    onfdict = {}\n",
    "    if len(routeList) > 0: #if detailed/standard use appropriate agg field to graph\n",
    "        xVal = 'ROUTE'\n",
    "    else:\n",
    "        xVal = 'metaMode'\n",
    "    # Make faceted graph for base and comparative scenario together    \n",
    "    if 'Comparative Model' in scenarios.keys():\n",
    "        scen2['compGraph']={}\n",
    "        for tod in scen['Base Model'].keys(): #add flag field so can smush both scenario tables into one\n",
    "            scen2['Base Model'][tod]['Scenario']='Base'\n",
    "            scen2['Comparative Model'][tod]['Scenario']='Comparative'\n",
    "            scen2['compGraph'][tod]=scen2['Base Model'][tod].append(scen2['Comparative Model'][tod]) #smoosh\n",
    "            \n",
    "        TODsums = scen2['compGraph']\n",
    "        \n",
    "        for z in TODsums.keys(): #make graphs (stacked bar)\n",
    "            #set up table so can use for facets (wide to long format and flag field)\n",
    "            lng = TODsums[z].drop(['DirectTransferOff','EgressOff','Off','On','WalkTransferOff'], axis = 1).melt(id_vars = [xVal, 'Scenario'], value_name = 'Count', ignore_index=False) #long to allow flag\n",
    "            #lng=lng.reset_index() #Scenario will be facet field\n",
    "            #make sure ids are strings for graphing purposes\n",
    "            lng[xVal] = lng[xVal].astype(str)\n",
    "            #make faceted stacked bar graphs (on and off dif graphs)\n",
    "            on_off = px.bar(lng, x = xVal, y = 'Count', color = 'variable', facet_col = 'Scenario',title='Base and Comparative Model: '+z+' Boardings')\n",
    "            #save graphs\n",
    "            onfdict[z] = on_off\n",
    "    else: #if only BASE\n",
    "        TODsums = scen2['Base Model']\n",
    "        for z in TODsums.keys(): #go through TOD\n",
    "            TODsums[z][xVal] = TODsums[z][xVal].astype(str) #make safe for graphing\n",
    "            onfdict[z] = px.bar(TODsums[z], x=xVal, y=['DirectTransferOn','DriveAccessOn','WalkAccessOn','WalkTransferOn'],  \n",
    "                               title='Base Model '+z+' Boardings') #graph!\n",
    "    return onfdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diftab(scen):\n",
    "#make difference tables\n",
    "    if len(routeList) > 0: #if detailed/standard use appropriate agg field to graph\n",
    "        xVal = 'ROUTE'\n",
    "    else:\n",
    "        xVal = 'metaMode'\n",
    "    if len(scenarios.keys()) ==  2:#if two scenarios\n",
    "        for z in TODsums.keys(): #for each TOD\n",
    "            #take the difference (and replace for TOD in the global TODsums)\n",
    "            TODsums[z] = (scen['Base Model'][z].set_index(xVal).drop('Scenario', axis=1) - scen['Comparative Model'][z].set_index(xVal).drop('Scenario', axis=1)).reset_index()\n",
    "            #make sure ids are strings for graphing purposes\n",
    "            TODsums[z][xVal] = TODsums[z][xVal].astype(str)\n",
    "            onfdict[z] = px.bar(TODsums[z], x=xVal, y=['DirectTransferOn','DriveAccessOn','WalkAccessOn','WalkTransferOn'],  \n",
    "                               title='Difference in '+z+' Boardings')\n",
    "    scen['Difference'] = [TODsums, onfdict] #add difference data and graphs to scen dict\n",
    "    return scen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-regard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call things\n",
    "# write a SUPER FUNCTION!!! (which calls all functions)\n",
    "\n",
    "# 'scen' is a two-level dict in which full set of results are accumulated.\n",
    "# Level 1 = scenario\n",
    "# Level 2 = tod\n",
    "scen = {}\n",
    "\n",
    "for g in scenarios.keys(): #run all these functions for each scenario\n",
    "    TODsums = import_transit_assignment(scenarios[g]) #get the total boarding per route per TOD\n",
    "    # Have to generate a route-to-mode=-to-metamode amapping table for _each scenario_\n",
    "    # because the list of routes MAY NOT be the same for each scenario!\n",
    "    routemode = set_up_metamode_table(scenarios[g]) \n",
    "    TODsums = join_and_agg(TODsums, routemode) #aggregate by mode or route\n",
    "    scen[g] = TODsums\n",
    "    #make graphs\n",
    "scen['compGraph'] = plots(scen,g)  #package the data for showing graphs\n",
    "\n",
    "#this is just for getting the difference to happen\n",
    "if len(scenarios.keys())==2:\n",
    "    scen = diftab(scen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8633aa2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d160f670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b006329f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "contrary-naples",
   "metadata": {},
   "source": [
    "## Look at Results by TOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show AM Boardings\n",
    "scen['compGraph']['AM'].show()\n",
    "#if comparative, also show graphs of boarding differences (base - comparative) \n",
    "if len(scenarios.keys())==2:\n",
    "    scen['Difference'][1]['AM'].show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-addition",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show MD Boardings\n",
    "scen['compGraph']['MD'].show()\n",
    "#if comparative, also show graphs of boarding differences (base - comparative) \n",
    "if len(scenarios.keys())==2:\n",
    "    scen['Difference'][1]['MD'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show PM Boardings\n",
    "scen['compGraph']['PM'].show()\n",
    "#if comparative, also show graphs of boarding differences (base - comparative) \n",
    "if len(scenarios.keys())==2:\n",
    "    scen['Difference'][1]['PM'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show NT Boardings\n",
    "scen['compGraph']['NT'].show()\n",
    "#if comparative, also show graphs of boarding differences (base - comparative) \n",
    "if len(scenarios.keys())==2:\n",
    "    scen['Difference'][1]['NT'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show Daily Boardings\n",
    "scen['compGraph']['daily'].show()\n",
    "#if comparative, also show graphs of boarding differences (base - comparative) \n",
    "if len(scenarios.keys())==2:\n",
    "    scen['Difference'][1]['daily'].show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-modx_proto1]",
   "language": "python",
   "name": "conda-env-.conda-modx_proto1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
