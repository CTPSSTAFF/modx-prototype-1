{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "closing-fishing",
   "metadata": {},
   "source": [
    "# Transit Boardings Report for All Routes\n",
    "\n",
    "This notebook generates a single reporton the total boardings for a __all__ transit routes \n",
    "under a __user-specified scenario__. The \"universe\" of routes for which to generate a report\n",
    "is specified by  _all_ the \"\\*ONO.csv\" files produced for the specified scenario. \n",
    "\n",
    "The report includes the following details:\n",
    "* Commuter Rail boardings for 4 time periods and the daily total for:\n",
    "* * Newburyport/Rockport Line\n",
    "* * Haverhill Line\n",
    "* * Lowell Line\n",
    "* * Fitchburg Line\n",
    "* * Worcester Line\n",
    "* * Needham Line\n",
    "* * Franklin Line\n",
    "* * Providence/Stoughton Line\n",
    "* * Fairmount Line\n",
    "* * Middleborough Line\n",
    "* * Plymouth Line\n",
    "* * Greenbush Line\n",
    "* Rapid Transit boardings for 4 time periods and the daily total for:\n",
    "* * Green Line Central\n",
    "* * Green Line B\n",
    "* * Green Line C\n",
    "* * Green Line D\n",
    "* * Green Line E\n",
    "* * Blue Line\n",
    "* * Orange Line\n",
    "* * Red Line\n",
    "* * Mattapan Line\n",
    "* MBTA bus boardings for 4 time periods and the daily total for:\n",
    "* * MBTA local bus\n",
    "* * MBTA express bus\n",
    "* * Silver Line Washington Street\n",
    "* * Silver Line Logan\n",
    "* * Silver Line \"Way\" (?)\n",
    "* MBTA boat boardings for 4 time periods, and the daily total\n",
    "* Logan shuttle/bus by \"type\" for 4 time periods, and the daily total\n",
    "* RTA bus boardings for 4 time periods and the daily total for\n",
    "* * Brockton RTA\n",
    "* * Cape Ann RTA\n",
    "* * Greater Attleboro RTA\n",
    "* * Lowell RTA\n",
    "* * Merrimack Valley RTA\n",
    "* * MetroWest RTA\n",
    "* * Worcester RTA\n",
    "* * Pioneer Valley RTA\n",
    "* * Southeastern Massachusetts RTA\n",
    "* Private bus line boardings for 4 time periods, and the daily total\n",
    "* MGH shuttle boardings for 4 time periods, and the daily total\n",
    "\n",
    "This notebook does _not_ produce a visualization of the report results.\n",
    "Use the __TBDX__ notebook for this purpose.\n",
    "\n",
    "To generate a report comparing the total boardings for all of transit routes \n",
    "under __two__ scenarios:\n",
    "* Run this notebook for the first scenario, capturing the output in a specified CSV file.\n",
    "* Run this notebook for the _second_ scenario, storing the output in a _second_ CSV file.\n",
    "* Run the notebook __TBDY__ to generate a comparative report of the two scenarios.\n",
    "\n",
    "### Organization of this notebook\n",
    "1. Import required packages\n",
    "2. Read config.py file to get paths to input and output directories\n",
    "3. User inputs\n",
    "  1. Specify scenario to run\n",
    "  2. Specify 'base' of output report file names\n",
    "4. Logic of the workbook _per se_\n",
    "  1. _calculate_total_daily_boardings_ - helper function for _import_transit_assignment_\n",
    "  2. _import_transit_assignment_ - reads all *ONO* CSV files for a time-of-day and sums them in a dataframe\n",
    "  3. _mode_to_metamode_ - maps TransCAD 'mode' to human-comprehensible 'meta-mode'; to be moved into _modxlib_.\n",
    "  4. _set_up_metamode_table_ - creates route-to-mode-to-metamode mapping table for all specified routes\n",
    "  5. _join_and_agg_ - aggregate results of running _import_transit_assignment_ according to the categories listed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from functools import reduce\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from plotly import tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f34efbd",
   "metadata": {},
   "source": [
    "###  User input required: Specity (and run) config.py file - get names of input and output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"S:/jupyter_notebooks/config.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0240a7bf",
   "metadata": {},
   "source": [
    "### User input required: Specify scenario to run, referencing relevant variable in config.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = base_scenario_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0485428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The contents of this cell are vestigial - kept for the moment for reference only.\n",
    "#\n",
    "# Reference data: CSV file containing list of _ALL_ transit routes:\n",
    "# *** This is JUST reference data - it's not used\n",
    "# all_transit_routes_csv_fn = \\\n",
    "# r'G:\\Regional_Modeling\\1A_Archives\\LRTP_2018\\2016 Scen 00_08March2019_MoDXoutputs\\Databases\\Statewide_Routes_2018S.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815321d3",
   "metadata": {},
   "source": [
    "### User input required: Specify 'base' of name of output CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files_base_name = 'my_transit_report'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda5b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_total_daily_boardings: Calculate the daily total across all time periods.\n",
    "#\n",
    "#    This calculation requires a bit of subtelty, because the number of rows in the four\n",
    "#    data frames produced by produced in the calling function is NOT necessarily the same. \n",
    "#    A brute-force apporach will not work, generally speaking.\n",
    "#    See comments in the code below for details.\n",
    "#\n",
    "# NOTE: This is a helper function for import_transit_assignment (q.v.)\n",
    "#   \n",
    "# Parameter: boardings_by_tod - a dict with the keys 'AM', 'MD', 'PM', and 'NT'\n",
    "#            for which the value of each key is a data frame containing the total\n",
    "#            boardings for the list of routes specified in the input CSV file.\n",
    "#\n",
    "# Return value: The input dict (boardings_by_tod) with an additional key 'daily'\n",
    "#               the value of which is a dataframe with the total daily boardings\n",
    "#               for all routes specified in the input CSV across all 4 time periods.\n",
    "#\n",
    "def calculate_total_daily_boardings(boardings_by_tod):\n",
    "    am_results = boardings_by_tod['AM']\n",
    "    md_results = boardings_by_tod['MD']\n",
    "    pm_results = boardings_by_tod['PM']\n",
    "    nt_results = boardings_by_tod['NT']\n",
    "    \n",
    "    # Compute the daily sums.\n",
    "    #\n",
    "    # Step 1: Join 'am' and 'md' dataframes\n",
    "    j1 = pd.merge(am_results, md_results, on=['ROUTE', 'STOP'], how='outer', suffixes=('_am', '_md'))\n",
    "    # Step 1.1 Replace NaN's with 0's\n",
    "    j1 = j1.fillna(0)\n",
    "\n",
    "    # Step 1.2 Compute the 'AM' + 'MD' sums\n",
    "    j1['DirectTransferOff'] = j1['DirectTransferOff_am'] + j1['DirectTransferOff_md']\n",
    "    j1['DirectTransferOn'] = j1['DirectTransferOn_am'] + j1['DirectTransferOn_md']\n",
    "    j1['DriveAccessOn'] = j1['DriveAccessOn_am'] + j1['DriveAccessOn_md']\n",
    "    j1['EgressOff'] = j1['EgressOff_am'] + j1['EgressOff_md']\n",
    "    j1['Off'] = j1['Off_am'] + j1['Off_md']\n",
    "    j1['On'] = j1['On_am'] + j1['On_md']\n",
    "    j1['WalkAccessOn'] = j1['WalkAccessOn_am'] + j1['WalkAccessOn_md'] \n",
    "    j1['WalkTransferOff'] = j1['WalkTransferOff_am'] + j1['WalkTransferOff_md']\n",
    "    j1['WalkTransferOn'] = j1['WalkTransferOn_am'] + j1['WalkTransferOn_md']\n",
    "\n",
    "    # Step 1.3: Drop un-needed columns\n",
    "    cols_to_drop = ['DirectTransferOff_am', 'DirectTransferOff_md',\n",
    "                    'DirectTransferOn_am', 'DirectTransferOn_md',\n",
    "                    'DriveAccessOn_am', 'DriveAccessOn_md',\n",
    "                    'EgressOff_am','EgressOff_md',\n",
    "                    'Off_am', 'Off_md',\n",
    "                    'On_am', 'On_md',\n",
    "                    'WalkAccessOn_am', 'WalkAccessOn_md',\n",
    "                    'WalkTransferOff_am', 'WalkTransferOff_md',\n",
    "                    'WalkTransferOn_am', 'WalkTransferOn_md'\n",
    "                    ]\n",
    "    j1 = j1.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Step 2: j2 - join 'pm' and 'nt' data frames\n",
    "    j2 = pd.merge(pm_results, nt_results, on=['ROUTE', 'STOP'], how='outer', suffixes=('_pm', '_nt'))\n",
    "    # Step 2.1: Replace NaN's with 0's\n",
    "    j2 = j2.fillna(0)\n",
    "\n",
    "    # Step 2.2: Compute the 'PM' + 'NT' sums\n",
    "    j2['DirectTransferOff'] = j2['DirectTransferOff_pm'] + j2['DirectTransferOff_nt']\n",
    "    j2['DirectTransferOn'] = j2['DirectTransferOn_pm'] + j2['DirectTransferOn_nt']\n",
    "    j2['DriveAccessOn'] = j2['DriveAccessOn_pm'] + j2['DriveAccessOn_nt']\n",
    "    j2['EgressOff'] = j2['EgressOff_pm'] + j2['EgressOff_nt']\n",
    "    j2['Off'] = j2['Off_pm'] + j2['Off_nt']\n",
    "    j2['On'] = j2['On_pm'] + j2['On_nt']\n",
    "    j2['WalkAccessOn'] = j2['WalkAccessOn_pm'] + j2['WalkAccessOn_nt'] \n",
    "    j2['WalkTransferOff'] = j2['WalkTransferOff_pm'] + j2['WalkTransferOff_nt']\n",
    "    j2['WalkTransferOn'] = j2['WalkTransferOn_pm'] + j2['WalkTransferOn_nt']\n",
    "\n",
    "    # Step 2.3: Drop un-needed columns\n",
    "    cols_to_drop = ['DirectTransferOff_pm', 'DirectTransferOff_nt',\n",
    "                    'DirectTransferOn_pm', 'DirectTransferOn_nt',\n",
    "                    'DriveAccessOn_pm', 'DriveAccessOn_nt',\n",
    "                    'EgressOff_pm','EgressOff_nt',\n",
    "                    'Off_pm', 'Off_nt',\n",
    "                    'On_pm', 'On_nt',\n",
    "                    'WalkAccessOn_pm', 'WalkAccessOn_nt',\n",
    "                    'WalkTransferOff_pm', 'WalkTransferOff_nt',\n",
    "                    'WalkTransferOn_pm', 'WalkTransferOn_nt'\n",
    "                    ]\n",
    "    j2 = j2.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Step 3: Join \"j1\" and \"j2\" to produce a dataframe with the daily totals\n",
    "    daily_df = pd.merge(j1, j2, on=['ROUTE', 'STOP'], how='outer', suffixes=('_j1', '_j2'))\n",
    "    # Step 3.1 : Replace any NaN's with 0's. This line _shouldn't_ be needed - just being extra cautious.\n",
    "    daily_df = daily_df.fillna(0)\n",
    "\n",
    "    # Step 3.2 : Compute THE daily sums\n",
    "    daily_df['DirectTransferOff'] = daily_df['DirectTransferOff_j1'] + daily_df['DirectTransferOff_j2']\n",
    "    daily_df['DirectTransferOn'] = daily_df['DirectTransferOn_j1'] + daily_df['DirectTransferOn_j2']\n",
    "    daily_df['DriveAccessOn'] = daily_df['DriveAccessOn_j1'] + daily_df['DriveAccessOn_j2']\n",
    "    daily_df['EgressOff'] = daily_df['EgressOff_j1'] + daily_df['EgressOff_j2']\n",
    "    daily_df['Off'] = daily_df['Off_j1'] + daily_df['Off_j2']\n",
    "    daily_df['On'] = daily_df['On_j1'] + daily_df['On_j2']\n",
    "    daily_df['WalkAccessOn'] = daily_df['WalkAccessOn_j1'] + daily_df['WalkAccessOn_j2'] \n",
    "    daily_df['WalkTransferOff'] = daily_df['WalkTransferOff_j1'] + daily_df['WalkTransferOff_j2']\n",
    "    daily_df['WalkTransferOn'] = daily_df['WalkTransferOn_j1'] + daily_df['WalkTransferOn_j2']\n",
    "\n",
    "    # Step 3.3 : Drop un-needed columns\n",
    "    cols_to_drop = ['DirectTransferOff_j1', 'DirectTransferOff_j2',\n",
    "                    'DirectTransferOn_j1', 'DirectTransferOn_j2',\n",
    "                    'DriveAccessOn_j1', 'DriveAccessOn_j2',\n",
    "                    'EgressOff_j1','EgressOff_j2',\n",
    "                    'Off_j1', 'Off_j2',\n",
    "                    'On_j1', 'On_j2',\n",
    "                    'WalkAccessOn_j1', 'WalkAccessOn_j2',\n",
    "                    'WalkTransferOff_j1', 'WalkTransferOff_j2',\n",
    "                    'WalkTransferOn_j1', 'WalkTransferOn_j2'\n",
    "                    ]\n",
    "    daily_df = daily_df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Finally, we've got the 'daily' total dataframe!\n",
    "    boardings_by_tod['daily'] = daily_df\n",
    "    return boardings_by_tod\n",
    "# end_def calculate_total_daily_boardings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463fd643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import_transit_assignment: Import transit assignment result CSV files for a given scenario.\n",
    "#\n",
    "# 1. Read all CSV files for each time period ('tod'), and caclculate the sums for each time period.\n",
    "#    Step 1 can be performed as a brute-force sum across all columns, since the number of rows in\n",
    "#    the CSVs (and thus the dataframes) for any given time period are all the same.\n",
    "#\n",
    "# 2. Calculate the daily total across all time periods.\n",
    "#    Step 2 requires a bit of subtelty, because the number of rows in the data frames produced in \n",
    "#    Step 1 is NOT necessarily the same. A brute-force apporach will not work, generally speaking.\n",
    "#    See comments in the code below for details.\n",
    "#    NOTE: This step is performed by the helper function calculate_total_daily_boardings.\n",
    "#\n",
    "# 3. Return value: a dict of the form:\n",
    "#    {'AM'    : dataframe with totals for the AM period,\n",
    "#     'MD'    : datafrme with totals for the MD period,\n",
    "#     'PM'    : dataframe with totals for the PM period,\n",
    "#     'NT'    : dataframe with totals for the NT period,\n",
    "#     'daily' : dataframe with totals for the entire day\n",
    "#   }\n",
    "# \n",
    "def import_transit_assignment(scenario):\n",
    "    base = scenario + r'out/'\n",
    "    tods = [\"AM\", \"MD\", \"PM\", \"NT\"]\n",
    "    # At the end of execution of this function, the dictionary variable'TODsums' will contain all the TOD summed results:\n",
    "    # one key-value-pair for each 'tod' AND the 'daily' total as well.\n",
    "    \n",
    "    # The dict 'TODsums' is the return value of this function.\n",
    "    TODsums = { 'AM' : None, 'MD' : None, 'PM' : None, 'NT' : None }\n",
    "\n",
    "    # Import CSV files and create sum tables for each T-O-D (a.k.a. 'time period').\n",
    "    for tod in tods:\n",
    "        # Get full paths to _all_ CSV files for the current t-o-d \n",
    "        x = tod + '/' \n",
    "        fq_csv_fns = glob.glob(os.path.join(base,x,r'*.csv'))\n",
    "        # 'tablist' : List of all the dataframes created from reading in the all the CSV files for the current t-o-d\n",
    "        tablist = []\n",
    "        for csv_file in fq_csv_fns:\n",
    "            # Read CSV file into dataframe, set indices, and append to 'tablist'\n",
    "            tablist.append(pd.read_csv(csv_file).set_index(['ROUTE','STOP']))\n",
    "        #\n",
    "  \n",
    "        # Sum the tables for the current TOD\n",
    "        TODsums[tod] = reduce(lambda a, b: a.add(b, fill_value=0), tablist)\n",
    "    # end_for over all tod's\n",
    "\n",
    "    TODsums =  calculate_total_daily_boardings(TODsums)\n",
    "    \n",
    "    # Ensure that the ROUTE and STOP columns of each dataframe in TODsums aren't indices.\n",
    "    for k in TODsums.keys():\n",
    "        TODsums[k] = TODsums[k].reset_index()\n",
    "    #\n",
    "    return TODsums\n",
    "# end_def import_transit_assignment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a295a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7349680b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3a8835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ace7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First crack at more meaningful route classification for transit report.\n",
    "#\n",
    "# Define data structure and function classify transit routes for reporting purposes.\n",
    "_classification_table = {\n",
    "    1:  'MBTA Local Bus',\n",
    "    2:  'MBTA Express Bus',\n",
    "    3:  'MBTA Express Bus' ,\n",
    "    4:  'Green Line',\n",
    "    5:  'Red Line',\n",
    "    6:  'Mattapan Trolley',\n",
    "    7:  'Orange Line',\n",
    "    8:  'Blue Line',\n",
    "    9:  'Commuter Rail',\n",
    "    10: 'Inner Harbor Ferry',\n",
    "    11: 'Other Ferries',\n",
    "    12: 'Silver Line',\n",
    "    13: 'Sliver Line',\n",
    "    14: 'Logan Express',\n",
    "    15: 'Logan Shuttle',\n",
    "    16: 'MGH and Other Shuttles',\n",
    "    17: 'Brockton RTA Bus',\n",
    "    18: 'CATA Bus',\n",
    "    19: 'GATRA Bus',\n",
    "    20: 'Lowell RTA Bus',\n",
    "    21: 'Merrimack RTA Bus',\n",
    "    22: 'MetroWest RTA Bus',\n",
    "    23: 'Bloom Bus (private)',\n",
    "    24: 'C & J Bus (private)',\n",
    "    25: 'Boston Express Bus (private)',\n",
    "    26: 'Concord Coach Bus (private)',\n",
    "    27: 'Dattco Bus (private)',\n",
    "    28: 'Plymouth & Brockton Bus (private)',\n",
    "    29: 'Peter Pan Bus (private)',\n",
    "    30: 'Yankee Bus (private)',\n",
    "    31: 'Miscellaneous Bus Routes',\n",
    "    32: 'Commuter Rail',\n",
    "    33: 'Commuter Rail',\n",
    "    34: 'Commuter Rail',\n",
    "    35: 'Commuter Rail',\n",
    "    36: 'Commuter Rail',\n",
    "    37: 'Commuter Rail',\n",
    "    38: 'Commuter Rail',\n",
    "    39: 'Commuter Rail',\n",
    "    40: 'Commuter Rail',\n",
    "    41: 'SRTA Bus',\n",
    "    42: 'Worcester RTA Bus',\n",
    "    43: 'PVTA RTA Bus',\n",
    "    44: 'Unknown RTA Bus',\n",
    "    70: 'Walk' }\n",
    "\n",
    "def classify_green_line_route(row):\n",
    "    # Crude, first-crack implementation based on TDM19 'Routes_ID' field.\n",
    "    retval = 'Green Line - '\n",
    "    route_id = row['Routes_ID']\n",
    "    if route_id in [6034, 6035]:\n",
    "        retval += 'B'\n",
    "    elif route_id in [6032, 6033]:\n",
    "        retval += 'C'\n",
    "    elif route_id in [6036, 6037]:\n",
    "        retval += 'D'\n",
    "    elif route_id in [8393, 8394]:\n",
    "        retval += 'E'\n",
    "    elif route_id in [6038, 6039]:\n",
    "        retval += 'D (GLX)'\n",
    "    elif route_id in [6028, 6029]:\n",
    "        retval += 'E (GLX)'\n",
    "    else:\n",
    "        retval += 'UNKNONWN'\n",
    "    # end_if \n",
    "    return retval\n",
    "    \n",
    "def classify_silver_line_route(row):\n",
    "    # Crude, first-crack implementation based on Ed Bromage's 'Mode'\n",
    "    # and the TDM19 'Routes_ID' field.\n",
    "    retval = 'Silver Line - '\n",
    "    ed_mode = row['Mode']\n",
    "    if ed_mode == 13:\n",
    "        retval += 'Washington Street'\n",
    "    else:\n",
    "        # ed_mode == 12\n",
    "        route_id = row['Routes_ID']\n",
    "        if route_id in [6050, 6051]:\n",
    "            retval += 'Drydock'\n",
    "        elif route_id in [6055, 6056]:\n",
    "            retval += 'Logan Airport'\n",
    "        elif route_id in [6058, 6059, 8256, 8257, 8258, 8259, 8260, 8261]:\n",
    "            retval += 'Chelsea'\n",
    "        else:\n",
    "            retval += 'UNKNOWN'\n",
    "        # end_if\n",
    "    # end_if  \n",
    "    return retval\n",
    "    \n",
    "def classify_commuter_rail_route(row):\n",
    "    # Crude, first-crack implementation based on solely Ed Bromage's 'Mode'.\n",
    "    # This implementation could be moved into the calling logic, but is\n",
    "    # placed here in expectation that further refinement will be needed.\n",
    "    # Yes, this version could have been implemented with a lookup table...\n",
    "    ed_mode = str(row['Mode'])\n",
    "    retval = 'Commuter Rail - '\n",
    "    if ed_mode == 9:\n",
    "        retval += 'Fairmount Line'\n",
    "    elif ed_mode == 32:\n",
    "        retval += 'Beverly / Newburyport / Rockport Line'\n",
    "    elif ed_mode == 33:\n",
    "        retval +=  'Stoughton / Providence Line'\n",
    "    elif ed_mode == 34:\n",
    "        retval +=  'Kingston Line'\n",
    "    elif ed_mode == 35:\n",
    "        retval += 'Haverhill Line'\n",
    "    elif ed_mode == 36:\n",
    "        retval = temp + 'Lowell Line'\n",
    "    elif ed_mode == 37:\n",
    "        retval += 'Fitchburg Line'\n",
    "    elif ed_mode == 38:\n",
    "        retval += 'Framingham / Worcester Line'\n",
    "    elif ed_mode == 39:\n",
    "        retval += 'Needham Line'\n",
    "    elif ed_mode == 40:\n",
    "        retval += 'Franklin Line'\n",
    "    else:\n",
    "        retval += 'UNKNOWN'\n",
    "    # end_if\n",
    "    return retval\n",
    "# end_def classify_commuter_rail_route()\n",
    "\n",
    "def classify_transit_route(row):\n",
    "    retval = 'None'\n",
    "    eds_mode = row['Mode']\n",
    "    if eds_mode == 4:\n",
    "        retval = classify_green_line_route(row)\n",
    "    elif eds_mode in [12, 13]:\n",
    "        retval = classify_silver_line_route(row)\n",
    "    elif eds_mode in [9,34, 35, 36, 37, 38, 39, 40]:\n",
    "        retval = classify_commuter_rail_route(row)\n",
    "    elif eds_mode in _classification_table:\n",
    "        retval = _classification_table[eds_mode]\n",
    "    # end_if\n",
    "    return retval\n",
    "# end_def classify_transit_route()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107c5295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_table(scenario):\n",
    "    temp_df = pd.read_csv(scenario + r'Databases/Statewide_Routes_2018S.csv',\n",
    "                          usecols=[\"Routes_ID\", \"Mode\", \"Route_Name\"]).drop_duplicates()\n",
    "    temp_df.rename(columns={\"Route_Name\": \"TransCAD_Route_Name\"}, inplace=True)\n",
    "    temp_df['report_class'] = temp_df.apply(lambda x: classify_transit_route(x), axis=1)\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736922a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570657ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac57798d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11efcd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b6666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** This function will require modification to report on \"all\" routes by \"route classification\".\n",
    "# -- BK 10/12/2021\n",
    "\n",
    "def join_and_aggregate(TODSums, classification_df, classification_field):\n",
    "    for x in TODsums.keys():\n",
    "        # In 'routefile' the TransCAD route ID is found in the Route_ID field;\n",
    "        # In the TODsums dictionaries (generated from the '*ONO*' CSV files),\n",
    "        # the TransCAD route ID is in the ROUTE field.\n",
    "        # TODsums[x] = routes_df.merge(TODsums[x], how='outer', left_on='Route_ID', right_on='ROUTE')\n",
    "                       \n",
    "        # The following statement generates a route name that is intelligible by mere mortals\n",
    "        # from the funky MBTA route name (in the 'Route_Name' field of the routes_df dataframe.)\n",
    "        # We store this in a new column ('ROUTE_TEXT') rather than overwriting the contents of the 'ROUTE' column.\n",
    "        # TODsums[x]['ROUTE_TEXT'] = TODsums[x]['Route_Name'].str.split('.:()').str[0]\n",
    "        \n",
    "        # Join each 'TOD' dataframe to the classification table\n",
    "        TODsums[x] = classification_df.merge(TODsums[x], how='right', left_on='Routes_ID', right_on='ROUTE')\n",
    "        \n",
    "        # Aggregate by the classification field, and get the sum of boardings ('On's)\n",
    "        TODsums[x] = TODsums[x].groupby([classification_field])['On'].agg('sum').reset_index()\n",
    "    return TODsums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c23cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8771747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d01b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efc2deab",
   "metadata": {},
   "source": [
    "### Here begins the driver logic for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d3d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total boardings per route for each time period ('tod') and for the day as a whole.\n",
    "TODsums = import_transit_assignment(scenario) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b7fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TODsums['AM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28310da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b54d8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7e0f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa208d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the route classification table\n",
    "classification_table = create_classification_table(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592a278",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035c8047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e30679e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccadab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform aggregation: aggregate results by the 'report_class' field of the classification table.\n",
    "#\n",
    "TODsums = join_and_aggregate(TODsums, classification_table, 'report_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bafbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TODsums['PM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb19c12b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df2dba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99695637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96a4cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fully-qualified names of output CSV files.\n",
    "#\n",
    "# First, pluck name of scenario from last element of scenario directory name remove trailing '/'\n",
    "temp1 = scenario[0:len(scenario)-1]\n",
    "temp2 = os.path.split(temp1)\n",
    "# Get 'raw' scenario name: may have blanks. Ugh!\n",
    "raw_scenario_name = temp2[1]\n",
    "clean_scenario_name = raw_scenario_name.replace(' ', '_')\n",
    "#\n",
    "am_report_csv_fn = sandbox_dir + clean_scenario_name + '_am_transit_report.csv'\n",
    "md_report_csv_fn = sandbox_dir + clean_scenario_name + '_md_transit_report.csv'\n",
    "pm_report_csv_fn = sandbox_dir + clean_scenario_name + '_pm_transit_report.csv'\n",
    "nt_report_csv_fn = sandbox_dir + clean_scenario_name + '_nt_transit_report.csv'\n",
    "daily_report_csv_fn = sandbox_dir + clean_scenario_name + '_daily_transit_report.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa712847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write output CSV report files\n",
    "#\n",
    "TODsums['AM'].to_csv(am_report_csv_fn, sep=',')\n",
    "TODsums['MD'].to_csv(md_report_csv_fn, sep=',')\n",
    "TODsums['PM'].to_csv(pm_report_csv_fn, sep=',')\n",
    "TODsums['NT'].to_csv(nt_report_csv_fn, sep=',')\n",
    "TODsums['daily'].to_csv(daily_report_csv_fn, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-radiation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-victor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-modx_proto1_1]",
   "language": "python",
   "name": "conda-env-.conda-modx_proto1_1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
