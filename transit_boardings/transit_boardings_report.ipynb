{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "closing-fishing",
   "metadata": {},
   "source": [
    "# Transit Boardings Report for specified list of routes\n",
    "\n",
    "This notebook generates a single (logical) report, consisting of five physical reports (CSV files)\n",
    "on the total boardings for a __user-specified list__ of transit routes under a __user-specified scenario__.\n",
    "The individual reports generated are for\n",
    "1. total boardings during the AM period\n",
    "2. total boardings during the MD period\n",
    "3. total boardings during the PM period\n",
    "4. total boardings during the NT period\n",
    "5. toal boardings during the entire day (sum of the above)\n",
    "\n",
    "This notebook does not produce visualization of the report results.\n",
    "Use the __TBD1__ notebook for this purpose.\n",
    "\n",
    "To generate a report comparing the total boardings for a specified list of transit routes \n",
    "under __two__ scenarios:\n",
    "* Run this notebook for the first scenario, capturing the output in a specified set of CSV files\n",
    "* Run this notebook for the _second_ scenario, storing the output in a _second_ set of CSV files.\n",
    "* Run the notebook __TBD2__ to generate a comparative report of the two scenarios.\n",
    "\n",
    "### Organization of this notebook\n",
    "1. Import required packages\n",
    "2. Read config.py file to get paths to input and output directories\n",
    "3. User inputs\n",
    "  1. Specify scenario to run\n",
    "  2. Specify list of routes for which to generate report (specified in input CSV file)\n",
    "  3. Specify 'base' of output report file names\n",
    "  4. Specify aggregation method: aggregate by route or by meta-mode\n",
    "4. Logic of the workbook _per se_\n",
    "  1. _calculate_total_daily_boardings_ - helper function for _import_transit_assignment_\n",
    "  2. _import_transit_assignment_ - reads all *ONO* CSV files for a time-of-day and sums them in a dataframe\n",
    "  3. _mode_to_metamode_ - maps TransCAD 'mode' to human-comprehensible 'meta-mode'; to be moved into _modxlib_.\n",
    "  4. _set_up_metamode_table_ - creates route-to-mode-to-metamode mapping table for all specified routes\n",
    "  5. _join_and_agg_ - aggregate results of running _import_transit_assignment_ either by route or meta-mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from functools import reduce\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from plotly import tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f34efbd",
   "metadata": {},
   "source": [
    "###  User input required: Specity (and run) config.py file - get names of input and output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"S:/jupyter_notebooks/config.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0240a7bf",
   "metadata": {},
   "source": [
    "### User input required: Specify scenario to run, referencing relevant variable in config.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = base_scenario_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0485428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The contents of this cell are vestigial - kept for the moment for reference only.\n",
    "#\n",
    "# Reference data: CSV file containing list of _ALL_ transit routes:\n",
    "# *** This is JUST reference data - it's not used\n",
    "# all_transit_routes_csv_fn = \\\n",
    "# r'G:\\Regional_Modeling\\1A_Archives\\LRTP_2018\\2016 Scen 00_08March2019_MoDXoutputs\\Databases\\Statewide_Routes_2018S.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82076566",
   "metadata": {},
   "source": [
    "### User input required: supply name of CSV file with list of routes on which to report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761258d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of routes for which to generate a report is specified in the following input CSV file:\n",
    "#\n",
    "# The path if 'G:' is mounted as CTPS's Google Drive\n",
    "# routes_csv_fn = 'G:/Shared drives/TMD_TSA/Data/MoDX/DataStore/transit info.csv'\n",
    "#\n",
    "# The path if 'G:' is mounted to //lilliput/groups\n",
    "routes_csv_fn = 'G:/Data_Resources/DataStore//transit info.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815321d3",
   "metadata": {},
   "source": [
    "### User input required: Specify 'base' of name of output CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files_base_name = 'my_report'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a09af65",
   "metadata": {},
   "source": [
    "### User input required: Specify aggregation mode for report: aggregate by route or by meta-mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d778281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation_mode can either by 'ROUTE' or 'metaMode'; it defaults to 'metaMode'.\n",
    "aggregation_mode = 'metaMode'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda5b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_total_daily_boardings: Calculate the daily total across all time periods.\n",
    "\n",
    "#    This calculation requires a bit of subtelty, because the number of rows in the four\n",
    "#    data frames produced by produced in the calling function is NOT necessarily the same. \n",
    "#    A brute-force apporach will not work, generally speaking.\n",
    "#    See comments in the code below for details.\n",
    "#\n",
    "# NOTE: This is a helper function for import_transit_assignment (q.v.)\n",
    "#   \n",
    "# Parameter: boardings_by_tod - a dict with the keys 'AM', 'MD', 'PM', and 'NT'\n",
    "#            for which the value of each key is a data frame containing the total\n",
    "#            boardings for the list of routes specified in the input CSV file.\n",
    "#\n",
    "# Return value: The input dict (boardings_by_tod) with an additional key 'daily'\n",
    "#               the value of which is a dataframe with the total daily boardings\n",
    "#               for all routes specified in the input CSV across all 4 time periods.\n",
    "#\n",
    "def calculate_total_daily_boardings(boardings_by_tod):\n",
    "    am_results = boardings_by_tod['AM']\n",
    "    md_results = boardings_by_tod['MD']\n",
    "    pm_results = boardings_by_tod['PM']\n",
    "    nt_results = boardings_by_tod['NT']\n",
    "    \n",
    "    # Compute the daily sums.\n",
    "    #\n",
    "    # Step 1: Join 'am' and 'md' dataframes\n",
    "    j1 = pd.merge(am_results, md_results, on=['ROUTE', 'STOP'], how='outer', suffixes=('_am', '_md'))\n",
    "    # Step 1.1 Replace NaN's with 0's\n",
    "    j1 = j1.fillna(0)\n",
    "\n",
    "    # Step 1.2 Compute the 'AM' + 'MD' sums\n",
    "    j1['DirectTransferOff'] = j1['DirectTransferOff_am'] + j1['DirectTransferOff_md']\n",
    "    j1['DirectTransferOn'] = j1['DirectTransferOn_am'] + j1['DirectTransferOn_md']\n",
    "    j1['DriveAccessOn'] = j1['DriveAccessOn_am'] + j1['DriveAccessOn_md']\n",
    "    j1['EgressOff'] = j1['EgressOff_am'] + j1['EgressOff_md']\n",
    "    j1['Off'] = j1['Off_am'] + j1['Off_md']\n",
    "    j1['On'] = j1['On_am'] + j1['On_md']\n",
    "    j1['WalkAccessOn'] = j1['WalkAccessOn_am'] + j1['WalkAccessOn_md'] \n",
    "    j1['WalkTransferOff'] = j1['WalkTransferOff_am'] + j1['WalkTransferOff_md']\n",
    "    j1['WalkTransferOn'] = j1['WalkTransferOn_am'] + j1['WalkTransferOn_md']\n",
    "\n",
    "    # Step 1.3: Drop un-needed columns\n",
    "    cols_to_drop = ['DirectTransferOff_am', 'DirectTransferOff_md',\n",
    "                    'DirectTransferOn_am', 'DirectTransferOn_md',\n",
    "                    'DriveAccessOn_am', 'DriveAccessOn_md',\n",
    "                    'EgressOff_am','EgressOff_md',\n",
    "                    'Off_am', 'Off_md',\n",
    "                    'On_am', 'On_md',\n",
    "                    'WalkAccessOn_am', 'WalkAccessOn_md',\n",
    "                    'WalkTransferOff_am', 'WalkTransferOff_md',\n",
    "                    'WalkTransferOn_am', 'WalkTransferOn_md'\n",
    "                    ]\n",
    "    j1 = j1.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Step 2: j2 - join 'pm' and 'nt' data frames\n",
    "    j2 = pd.merge(pm_results, nt_results, on=['ROUTE', 'STOP'], how='outer', suffixes=('_pm', '_nt'))\n",
    "    # Step 2.1: Replace NaN's with 0's\n",
    "    j2 = j2.fillna(0)\n",
    "\n",
    "    # Step 2.2: Compute the 'PM' + 'NT' sums\n",
    "    j2['DirectTransferOff'] = j2['DirectTransferOff_pm'] + j2['DirectTransferOff_nt']\n",
    "    j2['DirectTransferOn'] = j2['DirectTransferOn_pm'] + j2['DirectTransferOn_nt']\n",
    "    j2['DriveAccessOn'] = j2['DriveAccessOn_pm'] + j2['DriveAccessOn_nt']\n",
    "    j2['EgressOff'] = j2['EgressOff_pm'] + j2['EgressOff_nt']\n",
    "    j2['Off'] = j2['Off_pm'] + j2['Off_nt']\n",
    "    j2['On'] = j2['On_pm'] + j2['On_nt']\n",
    "    j2['WalkAccessOn'] = j2['WalkAccessOn_pm'] + j2['WalkAccessOn_nt'] \n",
    "    j2['WalkTransferOff'] = j2['WalkTransferOff_pm'] + j2['WalkTransferOff_nt']\n",
    "    j2['WalkTransferOn'] = j2['WalkTransferOn_pm'] + j2['WalkTransferOn_nt']\n",
    "\n",
    "    # Step 2.3: Drop un-needed columns\n",
    "    cols_to_drop = ['DirectTransferOff_pm', 'DirectTransferOff_nt',\n",
    "                    'DirectTransferOn_pm', 'DirectTransferOn_nt',\n",
    "                    'DriveAccessOn_pm', 'DriveAccessOn_nt',\n",
    "                    'EgressOff_pm','EgressOff_nt',\n",
    "                    'Off_pm', 'Off_nt',\n",
    "                    'On_pm', 'On_nt',\n",
    "                    'WalkAccessOn_pm', 'WalkAccessOn_nt',\n",
    "                    'WalkTransferOff_pm', 'WalkTransferOff_nt',\n",
    "                    'WalkTransferOn_pm', 'WalkTransferOn_nt'\n",
    "                    ]\n",
    "    j2 = j2.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Step 3: Join \"j1\" and \"j2\" to produce a dataframe with the daily totals\n",
    "    daily_df = pd.merge(j1, j2, on=['ROUTE', 'STOP'], how='outer', suffixes=('_j1', '_j2'))\n",
    "    # Step 3.1 : Replace any NaN's with 0's. This line _shouldn't_ be needed - just being extra cautious.\n",
    "    daily_df = daily_df.fillna(0)\n",
    "\n",
    "    # Step 3.2 : Compute THE daily sums\n",
    "    daily_df['DirectTransferOff'] = daily_df['DirectTransferOff_j1'] + daily_df['DirectTransferOff_j2']\n",
    "    daily_df['DirectTransferOn'] = daily_df['DirectTransferOn_j1'] + daily_df['DirectTransferOn_j2']\n",
    "    daily_df['DriveAccessOn'] = daily_df['DriveAccessOn_j1'] + daily_df['DriveAccessOn_j2']\n",
    "    daily_df['EgressOff'] = daily_df['EgressOff_j1'] + daily_df['EgressOff_j2']\n",
    "    daily_df['Off'] = daily_df['Off_j1'] + daily_df['Off_j2']\n",
    "    daily_df['On'] = daily_df['On_j1'] + daily_df['On_j2']\n",
    "    daily_df['WalkAccessOn'] = daily_df['WalkAccessOn_j1'] + daily_df['WalkAccessOn_j2'] \n",
    "    daily_df['WalkTransferOff'] = daily_df['WalkTransferOff_j1'] + daily_df['WalkTransferOff_j2']\n",
    "    daily_df['WalkTransferOn'] = daily_df['WalkTransferOn_j1'] + daily_df['WalkTransferOn_j2']\n",
    "\n",
    "    # Step 3.3 : Drop un-needed columns\n",
    "    cols_to_drop = ['DirectTransferOff_j1', 'DirectTransferOff_j2',\n",
    "                    'DirectTransferOn_j1', 'DirectTransferOn_j2',\n",
    "                    'DriveAccessOn_j1', 'DriveAccessOn_j2',\n",
    "                    'EgressOff_j1','EgressOff_j2',\n",
    "                    'Off_j1', 'Off_j2',\n",
    "                    'On_j1', 'On_j2',\n",
    "                    'WalkAccessOn_j1', 'WalkAccessOn_j2',\n",
    "                    'WalkTransferOff_j1', 'WalkTransferOff_j2',\n",
    "                    'WalkTransferOn_j1', 'WalkTransferOn_j2'\n",
    "                    ]\n",
    "    daily_df = daily_df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Finally, we've got the 'daily' total dataframe!\n",
    "    boardings_by_tod['daily'] = daily_df\n",
    "    return boardings_by_tod\n",
    "# end_def calculate_total_daily_boardings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463fd643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import_transit_assignment: Import transit assignment result CSV files for a given scenario.\n",
    "#\n",
    "# 1. Read all CSV files for each time period ('tod'), and caclculate the sums for each time period.\n",
    "#    Step 1 can be performed as a brute-force sum across all columns, since the number of rows in\n",
    "#    the CSVs (and thus the dataframes) for any given time period are all the same.\n",
    "#\n",
    "# 2. Calculate the daily total across all time periods.\n",
    "#    Step 2 requires a bit of subtelty, because the number of rows in the data frames produced in \n",
    "#    Step 1 is NOT necessarily the same. A brute-force apporach will not work, generally speaking.\n",
    "#    See comments in the code below for details.\n",
    "#    NOTE: This step is performed by the helper function calculate_total_daily_boardings.\n",
    "#\n",
    "# 3. Return value: a dict of the form:\n",
    "#    {'AM'    : dataframe with totals for the AM period,\n",
    "#     'MD'    : datafrme with totals for the MD period,\n",
    "#     'PM'    : dataframe with totals for the PM period,\n",
    "#     'NT'    : dataframe with totals for the NT period,\n",
    "#     'daily' : dataframe with totals for the entire day\n",
    "#   }\n",
    "# \n",
    "def import_transit_assignment(scenario):\n",
    "    base = scenario + r'out/'\n",
    "    tods = [\"AM\", \"MD\", \"PM\", \"NT\"]\n",
    "    # At the end of execution of this function, the dictionary variable'TODsums' will contain all the TOD summed results:\n",
    "    # one key-value-pair for each 'tod' AND the 'daily' total as well.\n",
    "    \n",
    "    # The dict 'TODsums' is the return value of this function.\n",
    "    TODsums = { 'AM' : None, 'MD' : None, 'PM' : None, 'NT' : None }\n",
    "\n",
    "    # Import CSV files and create sum tables for each T-O-D (a.k.a. 'time period').\n",
    "    for tod in tods:\n",
    "        # Get full paths to _all_ CSV files for the current t-o-d \n",
    "        x = tod + '/' \n",
    "        fq_csv_fns = glob.glob(os.path.join(base,x,r'*.csv'))\n",
    "        # 'tablist' : List of all the dataframes created from reading in the all the CSV files for the current t-o-d\n",
    "        tablist = []\n",
    "        for csv_file in fq_csv_fns:\n",
    "            # Read CSV file into dataframe, set indices, and append to 'tablist'\n",
    "            tablist.append(pd.read_csv(csv_file).set_index(['ROUTE','STOP']))\n",
    "        #\n",
    "        \n",
    "        # Filter dataframe to include rows where 'ROUTE' is one of those selected to report on\n",
    "        for t in range(len(tablist)):\n",
    "            tablist[t] = tablist[t][tablist[t].index.get_level_values('ROUTE').isin(route_list)]\n",
    "        #\n",
    "        \n",
    "        # Sum the tables for the current TOD\n",
    "        TODsums[tod] = reduce(lambda a, b: a.add(b, fill_value=0), tablist)\n",
    "    # end_for over all tod's\n",
    "\n",
    "    TODsums =  calculate_total_daily_boardings(TODsums)\n",
    "    \n",
    "    # Ensure that the ROUTE and STOP columns of each dataframe in TODsums aren't indices.\n",
    "    for k in TODsums.keys():\n",
    "        TODsums[k] = TODsums[k].reset_index()\n",
    "    #\n",
    "    return TODsums\n",
    "# end_def import_transit_assignment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea10b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data structure and function to map a TransCAD 'Mode' to the corresponding 'Meta-mode'\n",
    "_mode_to_metamode_mapping_table = {\n",
    "    1:  'MBTA_Bus',\n",
    "    2:  'MBTA_Bus',\n",
    "    3:  'MBTA_Bus' ,\n",
    "    4:  'Light_Rail',\n",
    "    5:  'Heavy_Rail',\n",
    "    6:  'Heavy_Rail',\n",
    "    7:  'Heavy_Rail',\n",
    "    8:  'Heavy_Rail',\n",
    "    9:  'Commuter_Rail',\n",
    "    10: 'Ferry',\n",
    "    11: 'Ferry',\n",
    "    12: 'Light_Rail',\n",
    "    13: 'Light_Rail',\n",
    "    14: 'Shuttle_Express',\n",
    "    15: 'Shuttle_Express',\n",
    "    16: 'Shuttle_Express',\n",
    "    17: 'RTA',\n",
    "    18: 'RTA',\n",
    "    19: 'RTA',\n",
    "    20: 'RTA',\n",
    "    21: 'RTA',\n",
    "    22: 'RTA',\n",
    "    23: 'Private',\n",
    "    24: 'Private',\n",
    "    25: 'Private',\n",
    "    26: 'Private',\n",
    "    27: 'Private',\n",
    "    28: 'Private',\n",
    "    29: 'Private',\n",
    "    30: 'Private',\n",
    "    31: 'Private',\n",
    "    32: 'Commuter_Rail',\n",
    "    33: 'Commuter_Rail',\n",
    "    34: 'Commuter_Rail',\n",
    "    35: 'Commuter_Rail',\n",
    "    36: 'Commuter_Rail',\n",
    "    37: 'Commuter_Rail',\n",
    "    38: 'Commuter_Rail',\n",
    "    39: 'Commuter_Rail',\n",
    "    40: 'Commuter_Rail',\n",
    "    41: 'RTA',\n",
    "    42: 'RTA',\n",
    "    43: 'RTA',\n",
    "    70: 'Walk' }\n",
    "\n",
    "def mode_to_metamode(mode):\n",
    "    retval = 'None'\n",
    "    if mode in _mode_to_metamode_mapping_table:\n",
    "        return _mode_to_metamode_mapping_table[mode]\n",
    "    # end_if\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11efcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_metamode_table(scenario):\n",
    "    routemode = pd.read_csv(scenario + r'Databases/Statewide_Routes_2018S.csv', \n",
    "                            usecols=[\"Routes_ID\", \"Mode\"]).drop_duplicates()\n",
    "    routemode['metaMode'] = routemode.apply(lambda x: mode_to_metamode(x['Mode']), axis=1)\n",
    "    return routemode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b6666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_and_aggregate(TODSums, agg_mode):\n",
    "    for x in TODsums.keys():\n",
    "        # In 'routefile' the TransCAD route ID is found in the Route_ID field;\n",
    "        # In the TODsums dictionaries (generated from the '*ONO*' CSV files),\n",
    "        # the TransCAD route ID is in the ROUTE field.\n",
    "        TODsums[x] = routes_df.merge(TODsums[x], how='outer', left_on='Route_ID', right_on='ROUTE')\n",
    "        # The following statement generates a route name that is intelligible by mere mortals\n",
    "        # from the funky MBTA route name (in the 'Route_Name' field of the routes_df dataframe.)\n",
    "        # We store this in a new column ('ROUTE_TEXT') rather than overwriting the contents of the 'ROUTE' column.\n",
    "        TODsums[x]['ROUTE_TEXT'] = TODsums[x]['Route_Name'].str.split('.:()').str[0]\n",
    "        # Join each 'TOD' dataframe to the route-mode-metamode mapping table\n",
    "        TODsums[x] = routemode.merge(TODsums[x], how='right', left_on='Routes_ID', right_on='Route_ID')\n",
    "        # *** The following line, from the original version of this notebook is applicable when\n",
    "        #     the user has _NOT_ specified a list of routes to report on. (This implicitly means\n",
    "        #     that a report should be generated for all routes listed in the *ONO* CSV files.)\n",
    "        #     This code is being kept here, but commented out, for reference purposes only.\n",
    "        #     -- BK 10/08/2021\n",
    "        # else:\n",
    "        #    TODsums[x] = routemode.merge(TODsums[x], how='right', left_on='Routes_ID', right_on='ROUTE')\n",
    "        #\n",
    "        # Sum all On/Off fields by the specified aggregation mode.\n",
    "        # Note: If the user has specified aggregation by route (aggregation_mode == 'ROUTE'),\n",
    "        #       be sure to actually aggregate on the ROUTE_TEXT field.\n",
    "        agg_mode = 'ROUTE_TEXT' if agg_mode == 'ROUTE' else agg_mode\n",
    "        TODsums[x] = TODsums[x].groupby([agg_mode])[['DirectTransferOff','DirectTransferOn',\n",
    "                                                     'DriveAccessOn','EgressOff','Off','On',\n",
    "                                                     'WalkAccessOn','WalkTransferOff','WalkTransferOn'\n",
    "                                                    ]].agg('sum').reset_index()\n",
    "    return TODsums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc2deab",
   "metadata": {},
   "source": [
    "### Here begins the driver logic for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149ebd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file with list of routes on which to report into a pandas dataframe:\n",
    "routes_df = pd.read_csv(routes_csv_fn)\n",
    "\n",
    "# Get the list of TransCAD route IDs for the routes to report on from the 'Route_ID' field of this dataframe:\n",
    "route_list = routes_df['Route_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d3d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total boardings per route for each time period ('tod') and for the day as a whole.\n",
    "TODsums = import_transit_assignment(scenario) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa208d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate route-to-mode-to-metamode mapping table.\n",
    "# Note: We have to do this for _each scenario_, because the list of routes may vary from one scenario to another!\n",
    "routemode = set_up_metamode_table(scenario) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99695637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform aggregation: aggregate results by route or by meta-mode, as specified by user.\n",
    "TODsums = join_and_aggregate(TODsums, aggregation_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96a4cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fully-qualified names of output CSV files.\n",
    "#\n",
    "# First, pluck name of scenario from last element of scenario directory name remove trailing '/'\n",
    "temp1 = scenario[0:len(scenario)-1]\n",
    "temp2 = os.path.split(temp1)\n",
    "# Get 'raw' scenario name: may have blanks. Ugh!\n",
    "raw_scenario_name = temp2[1]\n",
    "clean_scenario_name = raw_scenario_name.replace(' ', '_')\n",
    "#\n",
    "am_report_csv_fn = sandbox_dir + clean_scenario_name + '_am_transit_report.csv'\n",
    "md_report_csv_fn = sandbox_dir + clean_scenario_name + '_md_transit_report.csv'\n",
    "pm_report_csv_fn = sandbox_dir + clean_scenario_name + '_pm_transit_report.csv'\n",
    "nt_report_csv_fn = sandbox_dir + clean_scenario_name + '_nt_transit_report.csv'\n",
    "daily_report_csv_fn = sandbox_dir + clean_scenario_name + '_daily_transit_report.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa712847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write output CSV report files\n",
    "#\n",
    "TODsums['AM'].to_csv(am_report_csv_fn, sep=',')\n",
    "TODsums['MD'].to_csv(md_report_csv_fn, sep=',')\n",
    "TODsums['PM'].to_csv(pm_report_csv_fn, sep=',')\n",
    "TODsums['NT'].to_csv(nt_report_csv_fn, sep=',')\n",
    "TODsums['daily'].to_csv(daily_report_csv_fn, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-radiation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-victor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-modx_proto1_1]",
   "language": "python",
   "name": "conda-env-.conda-modx_proto1_1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
