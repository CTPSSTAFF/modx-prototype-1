{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "closing-fishing",
   "metadata": {},
   "source": [
    "# Light-rail Rapid Transit Report\n",
    "\n",
    "# Note: This is a work-in-progress as of 25 October 2021.\n",
    "#            It is under development, and NOT for general use!\n",
    "\n",
    "This notebook generates a single summary report on the total boardings by station for all MBTA light rail rapid transit routes, i.e., the Green Line (Branches B, C, D, and E) and the Mattapan Trolley. \n",
    "\n",
    "This notebook does _not_ produce a visualization of the report results.\n",
    "Use the __TBD_X__ notebook for this purpose.\n",
    "\n",
    "To generate a report comparing the total boardings for all heavy rail rapid transit routes \n",
    "under __two__ scenarios:\n",
    "* Run this notebook for the first scenario, capturing the output in a specified CSV file.\n",
    "* Run this notebook for the _second_ scenario, storing the output in a _second_ CSV file.\n",
    "* Run the notebook __TBDY__ to generate a comparative report of the two scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f34efbd",
   "metadata": {},
   "source": [
    "###  User input required: Specify (and run) config.py file - get names of input and output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"S:/jupyter_notebooks/config.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0240a7bf",
   "metadata": {},
   "source": [
    "### User input required: Specify scenario to run, referencing relevant variable in config.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = base_scenario_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815321d3",
   "metadata": {},
   "source": [
    "### User input required: Specify 'suffix' (after scenario name, before '.csv' suffix)\n",
    "The full output CSV file name consists of:\n",
    "1. The path to your \"sandbox directory\", specified in config.py\n",
    "2. The scenario name (with blanks replaced by underscores)\n",
    "3. The filename 'suffix' you specify in the next cell\n",
    "4. '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv_fn_suffix = 'light_rail_rt_report_25Oct2021'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b6f33d",
   "metadata": {},
   "source": [
    "### Debug/trace flag: Principally for use during development, testing, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f51ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_trace_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e084aa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_csv(df, filename):\n",
    "    df.to_csv(sandbox_dir + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da29945",
   "metadata": {},
   "source": [
    "### Beginning of code for \"library\" routines - to be moved into modxlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda5b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_total_daily_boardings: Calculate the daily total across all time periods.\n",
    "#\n",
    "#    This calculation requires a bit of subtelty, because the number of rows in the four\n",
    "#    data frames produced by produced in the calling function is NOT necessarily the same. \n",
    "#    A brute-force apporach will not work, generally speaking.\n",
    "#    See comments in the code below for details.\n",
    "#\n",
    "# NOTE: This is a helper function for import_transit_assignment (q.v.)\n",
    "#   \n",
    "# Parameter: boardings_by_tod - a dict with the keys 'AM', 'MD', 'PM', and 'NT'\n",
    "#            for which the value of each key is a data frame containing the total\n",
    "#            boardings for the list of routes specified in the input CSV file.\n",
    "#\n",
    "# Return value: The input dict (boardings_by_tod) with an additional key 'daily'\n",
    "#               the value of which is a dataframe with the total daily boardings\n",
    "#               for all routes specified in the input CSV across all 4 time periods.\n",
    "#\n",
    "def calculate_total_daily_boardings(boardings_by_tod):\n",
    "    am_results = boardings_by_tod['AM']\n",
    "    md_results = boardings_by_tod['MD']\n",
    "    pm_results = boardings_by_tod['PM']\n",
    "    nt_results = boardings_by_tod['NT']\n",
    "    \n",
    "    # Compute the daily sums.\n",
    "    #\n",
    "    # Step 1: Join 'am' and 'md' dataframes\n",
    "    j1 = pd.merge(am_results, md_results, on=['ROUTE', 'STOP'], how='outer', suffixes=('_am', '_md'))\n",
    "    # Step 1.1 Replace NaN's with 0's\n",
    "    j1 = j1.fillna(0)\n",
    "\n",
    "    # Step 1.2 Compute the 'AM' + 'MD' sums\n",
    "    j1['DirectTransferOff'] = j1['DirectTransferOff_am'] + j1['DirectTransferOff_md']\n",
    "    j1['DirectTransferOn'] = j1['DirectTransferOn_am'] + j1['DirectTransferOn_md']\n",
    "    j1['DriveAccessOn'] = j1['DriveAccessOn_am'] + j1['DriveAccessOn_md']\n",
    "    j1['EgressOff'] = j1['EgressOff_am'] + j1['EgressOff_md']\n",
    "    j1['Off'] = j1['Off_am'] + j1['Off_md']\n",
    "    j1['On'] = j1['On_am'] + j1['On_md']\n",
    "    j1['WalkAccessOn'] = j1['WalkAccessOn_am'] + j1['WalkAccessOn_md'] \n",
    "    j1['WalkTransferOff'] = j1['WalkTransferOff_am'] + j1['WalkTransferOff_md']\n",
    "    j1['WalkTransferOn'] = j1['WalkTransferOn_am'] + j1['WalkTransferOn_md']\n",
    "\n",
    "    # Step 1.3: Drop un-needed columns\n",
    "    cols_to_drop = ['DirectTransferOff_am', 'DirectTransferOff_md',\n",
    "                    'DirectTransferOn_am', 'DirectTransferOn_md',\n",
    "                    'DriveAccessOn_am', 'DriveAccessOn_md',\n",
    "                    'EgressOff_am','EgressOff_md',\n",
    "                    'Off_am', 'Off_md',\n",
    "                    'On_am', 'On_md',\n",
    "                    'WalkAccessOn_am', 'WalkAccessOn_md',\n",
    "                    'WalkTransferOff_am', 'WalkTransferOff_md',\n",
    "                    'WalkTransferOn_am', 'WalkTransferOn_md'\n",
    "                    ]\n",
    "    j1 = j1.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Step 2: j2 - join 'pm' and 'nt' data frames\n",
    "    j2 = pd.merge(pm_results, nt_results, on=['ROUTE', 'STOP'], how='outer', suffixes=('_pm', '_nt'))\n",
    "    # Step 2.1: Replace NaN's with 0's\n",
    "    j2 = j2.fillna(0)\n",
    "\n",
    "    # Step 2.2: Compute the 'PM' + 'NT' sums\n",
    "    j2['DirectTransferOff'] = j2['DirectTransferOff_pm'] + j2['DirectTransferOff_nt']\n",
    "    j2['DirectTransferOn'] = j2['DirectTransferOn_pm'] + j2['DirectTransferOn_nt']\n",
    "    j2['DriveAccessOn'] = j2['DriveAccessOn_pm'] + j2['DriveAccessOn_nt']\n",
    "    j2['EgressOff'] = j2['EgressOff_pm'] + j2['EgressOff_nt']\n",
    "    j2['Off'] = j2['Off_pm'] + j2['Off_nt']\n",
    "    j2['On'] = j2['On_pm'] + j2['On_nt']\n",
    "    j2['WalkAccessOn'] = j2['WalkAccessOn_pm'] + j2['WalkAccessOn_nt'] \n",
    "    j2['WalkTransferOff'] = j2['WalkTransferOff_pm'] + j2['WalkTransferOff_nt']\n",
    "    j2['WalkTransferOn'] = j2['WalkTransferOn_pm'] + j2['WalkTransferOn_nt']\n",
    "\n",
    "    # Step 2.3: Drop un-needed columns\n",
    "    cols_to_drop = ['DirectTransferOff_pm', 'DirectTransferOff_nt',\n",
    "                    'DirectTransferOn_pm', 'DirectTransferOn_nt',\n",
    "                    'DriveAccessOn_pm', 'DriveAccessOn_nt',\n",
    "                    'EgressOff_pm','EgressOff_nt',\n",
    "                    'Off_pm', 'Off_nt',\n",
    "                    'On_pm', 'On_nt',\n",
    "                    'WalkAccessOn_pm', 'WalkAccessOn_nt',\n",
    "                    'WalkTransferOff_pm', 'WalkTransferOff_nt',\n",
    "                    'WalkTransferOn_pm', 'WalkTransferOn_nt'\n",
    "                    ]\n",
    "    j2 = j2.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Step 3: Join \"j1\" and \"j2\" to produce a dataframe with the daily totals\n",
    "    daily_df = pd.merge(j1, j2, on=['ROUTE', 'STOP'], how='outer', suffixes=('_j1', '_j2'))\n",
    "    # Step 3.1 : Replace any NaN's with 0's. This line _shouldn't_ be needed - just being extra cautious.\n",
    "    daily_df = daily_df.fillna(0)\n",
    "\n",
    "    # Step 3.2 : Compute THE daily sums\n",
    "    daily_df['DirectTransferOff'] = daily_df['DirectTransferOff_j1'] + daily_df['DirectTransferOff_j2']\n",
    "    daily_df['DirectTransferOn'] = daily_df['DirectTransferOn_j1'] + daily_df['DirectTransferOn_j2']\n",
    "    daily_df['DriveAccessOn'] = daily_df['DriveAccessOn_j1'] + daily_df['DriveAccessOn_j2']\n",
    "    daily_df['EgressOff'] = daily_df['EgressOff_j1'] + daily_df['EgressOff_j2']\n",
    "    daily_df['Off'] = daily_df['Off_j1'] + daily_df['Off_j2']\n",
    "    daily_df['On'] = daily_df['On_j1'] + daily_df['On_j2']\n",
    "    daily_df['WalkAccessOn'] = daily_df['WalkAccessOn_j1'] + daily_df['WalkAccessOn_j2'] \n",
    "    daily_df['WalkTransferOff'] = daily_df['WalkTransferOff_j1'] + daily_df['WalkTransferOff_j2']\n",
    "    daily_df['WalkTransferOn'] = daily_df['WalkTransferOn_j1'] + daily_df['WalkTransferOn_j2']\n",
    "\n",
    "    # Step 3.3 : Drop un-needed columns\n",
    "    cols_to_drop = ['DirectTransferOff_j1', 'DirectTransferOff_j2',\n",
    "                    'DirectTransferOn_j1', 'DirectTransferOn_j2',\n",
    "                    'DriveAccessOn_j1', 'DriveAccessOn_j2',\n",
    "                    'EgressOff_j1','EgressOff_j2',\n",
    "                    'Off_j1', 'Off_j2',\n",
    "                    'On_j1', 'On_j2',\n",
    "                    'WalkAccessOn_j1', 'WalkAccessOn_j2',\n",
    "                    'WalkTransferOff_j1', 'WalkTransferOff_j2',\n",
    "                    'WalkTransferOn_j1', 'WalkTransferOn_j2'\n",
    "                    ]\n",
    "    daily_df = daily_df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Finally, we've got the 'daily' total dataframe!\n",
    "    boardings_by_tod['daily'] = daily_df\n",
    "    return boardings_by_tod\n",
    "# end_def calculate_total_daily_boardings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463fd643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import_transit_assignment: Import transit assignment result CSV files for a given scenario.\n",
    "#\n",
    "# 1. Read all CSV files for each time period ('tod'), and caclculate the sums for each time period.\n",
    "#    Step 1 can be performed as a brute-force sum across all columns, since the number of rows in\n",
    "#    the CSVs (and thus the dataframes) for any given time period are all the same.\n",
    "#\n",
    "# 2. Calculate the daily total across all time periods.\n",
    "#    Step 2 requires a bit of subtelty, because the number of rows in the data frames produced in \n",
    "#    Step 1 is NOT necessarily the same. A brute-force apporach will not work, generally speaking.\n",
    "#    See comments in the code below for details.\n",
    "#    NOTE: This step is performed by the helper function calculate_total_daily_boardings.\n",
    "#\n",
    "# 3. Return value: a dict of the form:\n",
    "#    {'AM'    : dataframe with totals for the AM period,\n",
    "#     'MD'    : datafrme with totals for the MD period,\n",
    "#     'PM'    : dataframe with totals for the PM period,\n",
    "#     'NT'    : dataframe with totals for the NT period,\n",
    "#     'daily' : dataframe with totals for the entire day\n",
    "#   }\n",
    "# \n",
    "def import_transit_assignment(scenario):\n",
    "    base = scenario + r'out/'\n",
    "    tods = [\"AM\", \"MD\", \"PM\", \"NT\"]\n",
    "    # At the end of execution of this function, the dictionary variable'TODsums' will contain all the TOD summed results:\n",
    "    # one key-value-pair for each 'tod' AND the 'daily' total as well.\n",
    "    \n",
    "    # The dict 'TODsums' is the return value of this function.\n",
    "    TODsums = { 'AM' : None, 'MD' : None, 'PM' : None, 'NT' : None }\n",
    "\n",
    "    # Import CSV files and create sum tables for each T-O-D (a.k.a. 'time period').\n",
    "    for tod in tods:\n",
    "        # Get full paths to _all_ CSV files for the current t-o-d.\n",
    "        x = tod + '/' \n",
    "        fq_csv_fns = glob.glob(os.path.join(base,x,r'*.csv'))\n",
    "        \n",
    "        # 'tablist' : List of all the dataframes created from reading in the all the CSV files for the current t-o-d\n",
    "        tablist = []\n",
    "        for csv_file in fq_csv_fns:\n",
    "            # Read CSV file into dataframe, set indices, and append to 'tablist'\n",
    "            tablist.append(pd.read_csv(csv_file).set_index(['ROUTE','STOP']))\n",
    "        #\n",
    "  \n",
    "        # Sum the tables for the current TOD\n",
    "        TODsums[tod] = reduce(lambda a, b: a.add(b, fill_value=0), tablist)\n",
    "    # end_for over all tod's\n",
    "\n",
    "    TODsums =  calculate_total_daily_boardings(TODsums)\n",
    "    \n",
    "    # Ensure that the ROUTE and STOP columns of each dataframe in TODsums aren't indices.\n",
    "    for k in TODsums.keys():\n",
    "        TODsums[k] = TODsums[k].reset_index()\n",
    "    #\n",
    "    return TODsums\n",
    "# end_def import_transit_assignment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ace7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First crack at more meaningful route classification for transit report.\n",
    "# WARNING / NOTE: This classification is specific to TDM19 and will be replaced for TDM23!!!\n",
    "#\n",
    "# Define data structure and function to classify transit routes for reporting purposes.\n",
    "_classification_table = {\n",
    "    1:  'MBTA Bus - Local Bus',\n",
    "    2:  'MBTA Bus - Inner Express Bus',\n",
    "    3:  'MBTA Bus - Outer Express Bus' ,\n",
    "    4:  'Green Line',\n",
    "    5:  'Red Line',\n",
    "    6:  'Mattapan Trolley',\n",
    "    7:  'Orange Line',\n",
    "    8:  'Blue Line',\n",
    "    9:  'Commuter Rail - Fairmount Line',\n",
    "    10: 'Ferries - Inner Harbor',\n",
    "    11: 'Ferries - Outer Harbor',\n",
    "    12: 'Silver Line',\n",
    "    13: 'Sliver Line',\n",
    "    14: 'Logan Express',\n",
    "    15: 'Logan Shuttle',\n",
    "    16: 'MGH and Other Shuttles',\n",
    "    17: 'RTA Bus - Brockton RTA',\n",
    "    18: 'RTA Bus - CATA RTA',\n",
    "    19: 'RTA Bus - GATRA RTA',\n",
    "    20: 'RTA Bus - Lowell RTA',\n",
    "    21: 'RTA Bus - Merrimack RTA',\n",
    "    22: 'RTA Bus - MetroWest RTA',\n",
    "    23: 'Private Bus - Bloom',\n",
    "    24: 'Private Bus - C & J Bus',\n",
    "    25: 'Private Bus - Cavalier Bus',\n",
    "    26: 'Private Bus - Concord Coach',\n",
    "    27: 'Private Bus - Dattco Bus',\n",
    "    28: 'Private Bus - Plymouth & Brockton',\n",
    "    29: 'Private Bus - Peter Pan',\n",
    "    30: 'Private Bus - Yankee',\n",
    "    31: 'MBTA Subsidized Bus Routes',\n",
    "    32: 'Commuter Rail - Beverly / Newburyport / Rockport Line',\n",
    "    33: 'Commuter Rail - Stoughton / Providence Line',\n",
    "    34: 'Commuter Rail - Greenbush / Plymouth / Kingston / Middleborough Line',\n",
    "    35: 'Commuter Rail - Haverhill Line',\n",
    "    36: 'Commuter Rail - Lowell Line',\n",
    "    37: 'Commuter Rail - Fitchburg Line',\n",
    "    38: 'Commuter Rail - Framingham / Worcester Line',\n",
    "    39: 'Commuter Rail - Needham Line',\n",
    "    40: 'Commuter Rail - Franklin Line',\n",
    "    41: 'RTA Bus - SRTA RTA',\n",
    "    42: 'RTA Bus - Worcester RTA',\n",
    "    43: 'RTA Bus- Pioneer Valley RTA',\n",
    "    70: 'Walk' }\n",
    "\n",
    "def classify_green_line_route(row):\n",
    "    # Crude, first-crack implementation based on TDM19 'Routes_ID' field.\n",
    "    retval = 'Rapid Transit - Green Line '\n",
    "    route_id = row['Routes_ID']\n",
    "    if route_id in [6034, 6035]:\n",
    "        retval += 'B Branch'\n",
    "    elif route_id in [6032, 6033]:\n",
    "        retval += 'C Branch'\n",
    "    elif route_id in [6036, 6037]:\n",
    "        retval += 'D Branch'\n",
    "    elif route_id in [8393, 8394]:\n",
    "        retval += 'E Branch'\n",
    "    elif route_id in [6038, 6039]:\n",
    "        retval += 'D Branch (GLX)'\n",
    "    elif route_id in [6028, 6029]:\n",
    "        retval += 'E Branch (GLX)'\n",
    "    else:\n",
    "        retval += 'UNKNONWN'\n",
    "    # end_if \n",
    "    return retval\n",
    "    \n",
    "def classify_silver_line_route(row):\n",
    "    # Crude, first-crack implementation based on Ed Bromage's 'Mode'\n",
    "    # and the TDM19 'Routes_ID' field.\n",
    "    retval = 'Silver Line - '\n",
    "    ed_mode = row['Mode']\n",
    "    route_id = row['Routes_ID']\n",
    "    if ed_mode == 13:\n",
    "        if route_id in [8235, 8262]:\n",
    "            retval += 'SL4'\n",
    "        elif route_id in [8234, 8263]:\n",
    "            retval += 'SL5'\n",
    "        else:\n",
    "            retval += 'UNKNOWN ' + 'Mode = ' + str(ed_mode) + ' Routes_ID = ' + str(route_id)\n",
    "    else:\n",
    "        # ed_mode == 12\n",
    "        if route_id in [6055, 6056]:\n",
    "            retval += 'SL1'\n",
    "        elif route_id in [8256, 8257, 8258, 8259, 8260, 8261]:\n",
    "            retval += 'SL2'\n",
    "        elif route_id in [6058, 6059]:\n",
    "            retval += 'SL3'\n",
    "        elif route_id in [6050, 6051]:\n",
    "            retval += 'to Sliver Line Way'\n",
    "        else:\n",
    "            retval += 'UNKNOWN ' + 'Mode = ' + str(ed_mode) + ' Routes_ID = ' + str(route_id)\n",
    "        # end_if\n",
    "    # end_if  \n",
    "    return retval\n",
    "    \n",
    "def classify_commuter_rail_route(row):\n",
    "    # Crude, first-crack implementation based on solely Ed Bromage's 'Mode'.\n",
    "    # This implementation could be moved into the calling logic, but is\n",
    "    # placed here in expectation that further refinement will be needed.\n",
    "    # Yes, this version could have been implemented with a lookup table...\n",
    "    ed_mode = row['Mode']\n",
    "    retval = 'Commuter Rail - '\n",
    "    if ed_mode == 9:\n",
    "        retval += 'Fairmount Line'\n",
    "    elif ed_mode == 32:\n",
    "        retval += 'Beverly / Newburyport / Rockport Line'\n",
    "    elif ed_mode == 33:\n",
    "        retval += 'Stoughton / Providence Line'\n",
    "    elif ed_mode == 34:\n",
    "        retval += 'Greenbush / Plymouth / Kingston / Middleborough Line'\n",
    "    elif ed_mode == 35:\n",
    "        retval += 'Haverhill Line'\n",
    "    elif ed_mode == 36:\n",
    "        retval +='Lowell Line'\n",
    "    elif ed_mode == 37:\n",
    "        retval += 'Fitchburg Line'\n",
    "    elif ed_mode == 38:\n",
    "        retval += 'Framingham / Worcester Line'\n",
    "    elif ed_mode == 39:\n",
    "        retval += 'Needham Line'\n",
    "    elif ed_mode == 40:\n",
    "        retval += 'Franklin Line'\n",
    "    # end_if\n",
    "    return retval\n",
    "# end_def classify_commuter_rail_route()\n",
    "\n",
    "def classify_transit_route(row):\n",
    "    retval = 'None'\n",
    "    eds_mode = row['Mode']\n",
    "    if eds_mode == 4:\n",
    "        retval = classify_green_line_route(row)\n",
    "    elif eds_mode in [12, 13]:\n",
    "        retval = classify_silver_line_route(row)\n",
    "    # For the commuter rail lines: for now, just use the brute-force 'Mode' number.\n",
    "    # elif eds_mode in [9, 34, 35, 36, 37, 38, 39, 40]:\n",
    "    #    retval = classify_commuter_rail_route(row)\n",
    "    elif eds_mode in _classification_table:\n",
    "        retval = _classification_table[eds_mode]\n",
    "    else:\n",
    "        retval = 'UNKNOWN'\n",
    "    # end_if\n",
    "    return retval\n",
    "# end_def classify_transit_route()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107c5295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In spite of its name the 'Statewide_Routes' file is in reality a Stops file.\n",
    "# \n",
    "def load_stops_table(scenario):\n",
    "    temp_df = pd.read_csv(scenario + r'Databases/Statewide_Routes_2018S.csv',\n",
    "                          usecols=[\"Mode\", \"Routes_ID\", \"Route_Name\", \"STOP_ID\", \"STOP_Name\"]).drop_duplicates()\n",
    "    temp_df.rename(columns={\"Route_Name\": \"TransCAD_Route_Name\"}, inplace=True)\n",
    "    temp_df['report_class'] = temp_df.apply(lambda x: classify_transit_route(x), axis=1)\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b95d059",
   "metadata": {},
   "source": [
    "### End of \"library\" code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc2deab",
   "metadata": {},
   "source": [
    "### Here begins the driver logic for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d3d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total boardings per {route,stop} for each time period ('tod') and for the day as a whole.\n",
    "TODsums = import_transit_assignment(scenario) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81782164",
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug_trace_flag:\n",
    "    TODsums['daily']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd7d408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the route classification table\n",
    "classification_table = load_stops_table(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8db3b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug_trace_flag:\n",
    "    classification_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993650d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to be able to filter the TODsums data (boarding data) by line or stop,\n",
    "# Join each of the 'TODsums' dataframes (containing the ONO data) to the stops table\n",
    "# on TODsums[x].ROUTE <===> stops_table.Routes_ID\n",
    "for tod in TODsums.keys():\n",
    "    TODsums[tod] = TODsums[tod].merge(classification_table, \n",
    "                                      how='left', left_on='STOP', right_on='STOP_ID')\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e8fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug_trace_flag:\n",
    "    TODsums['daily']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3db8043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'ONO' data for the light rapid transit lines from the entire blob of ONO data.\n",
    "#\n",
    "ono_data = {}\n",
    "for k in TODsums.keys():\n",
    "    ono_data[k] = None\n",
    "#\n",
    "#\n",
    "# The following doesn't work - These (simple) strings don't match Mode names in TransCAD! Ugh! :-(\n",
    "heavy_rail_rt_route_names =  ['Green Line', 'Mattapan Trolley'] \n",
    "#\n",
    "# Warning: Using a QUICK HACK instead!!! \n",
    "# Using Ed Bromage's 'Mode' number to filter!!! :-( :-( :-(\n",
    "light_rail_rt_bromage_mode_ids = [4, 6]\n",
    "for tod in TODsums.keys():\n",
    "    boolean_series = TODsums[tod].Mode.isin(light_rail_rt_bromage_mode_ids)\n",
    "    ono_data[tod] = TODsums[tod][boolean_series]\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b23c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug_trace_flag:\n",
    "    ono_data['daily']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d906da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb37e530",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code beyond this point will need to be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5113a583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b548d2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns not to confound the 'groupby' operation --- MAY NOT BE NEEDED\n",
    "for tod in ono_data.keys():\n",
    "    ono_data[tod] = \\\n",
    "        ono_data[tod].drop(['ROUTE', 'Routes_ID', 'STOP', 'STOP_ID', 'TransCAD_Route_Name', 'Mode'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f60c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug_trace_flag:\n",
    "    ono_data['daily']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed4262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas appears to have trouble grouping-by more than one-string valued field.\n",
    "# We first group-by STOP_Name, and subsequently work back to the \"route\" name.\n",
    "grouped_ono_data = {}\n",
    "for tod in ono_data.keys():\n",
    "    grouped_ono_data[tod] = None\n",
    "#\n",
    "for tod in ono_data.keys():\n",
    "    grouped_ono_data[tod] = ono_data[tod].groupby(ono_data[tod]['STOP_Name']).sum()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be44009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug_trace_flag:\n",
    "    grouped_ono_data['daily']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8718f8f5",
   "metadata": {},
   "source": [
    "## Prepare final data frame for output to CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf70dfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "am_df = grouped_ono_data['AM']\n",
    "md_df = grouped_ono_data['MD']\n",
    "pm_df = grouped_ono_data['PM']\n",
    "nt_df = grouped_ono_data['NT']\n",
    "daily_df = grouped_ono_data['daily']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d694616",
   "metadata": {},
   "outputs": [],
   "source": [
    "j1 = pd.merge(am_df, md_df, on=['STOP_Name'], suffixes=('_am', '_md'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03ecc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "j2 = pd.merge(pm_df, nt_df, on=['STOP_Name'], suffixes=('_pm', '_nt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3a7f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "j3 = pd.merge(j1, j2, on=['STOP_Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0037a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "j4 = pd.merge(daily_df, j3, on=['STOP_Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d91f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create STOP_Name to (ersatz) \"route name\" mapping\n",
    "# in order to circumvent apparent Pandas issue with group-by on multiple string-valued fields.\n",
    "temp_df1 = ono_data['daily']\n",
    "temp_df2 = temp_df1[['STOP_Name', 'report_class']] \n",
    "stop_to_route_mapping = temp_df2.drop_duplicates()\n",
    "if debug_trace_flag:\n",
    "    stop_to_route_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb6f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug_trace_flag:\n",
    "    dump_csv(stop_to_route_mapping, \"stop_to_route_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0f94cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "j5 = j4.merge(stop_to_route_mapping, how=\"left\", left_on=\"STOP_Name\", right_on=\"STOP_Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373f750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = j5.rename(columns={\"report_class\": \"Line\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48650df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the data frame first by line (then, implicitly, by station.)\n",
    "red_df = temp_df[temp_df.Line == 'Red Line']\n",
    "orange_df = temp_df[temp_df.Line == 'Orange Line']\n",
    "blue_df = temp_df[temp_df.Line == 'Blue Line']\n",
    "final_df = pd.concat([red_df, orange_df, blue_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96bbca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6422307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e63d3fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9c9ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set final order of columns in preparation for writing dataframe out to CSV.\n",
    "final_column_order = ['Line', 'STOP_Name', \n",
    "                      'DirectTransferOff', 'DirectTransferOn', 'DriveAccessOn',\n",
    "                      'EgressOff', 'Off', 'On', 'WalkAccessOn', 'WalkTransferOff', 'WalkTransferOn', \n",
    "                      'DirectTransferOff_am', 'DirectTransferOn_am',\n",
    "                      'DriveAccessOn_am', 'EgressOff_am', 'Off_am', 'On_am',\n",
    "                      'WalkAccessOn_am', 'WalkTransferOff_am', 'WalkTransferOn_am',\n",
    "                      'DirectTransferOff_md', 'DirectTransferOn_md', 'DriveAccessOn_md',\n",
    "                      'EgressOff_md', 'Off_md', 'On_md', 'WalkAccessOn_md',\n",
    "                      'WalkTransferOff_md', 'WalkTransferOn_md', 'DirectTransferOff_pm',\n",
    "                      'DirectTransferOn_pm', 'DriveAccessOn_pm', 'EgressOff_pm', 'Off_pm',\n",
    "                      'On_pm', 'WalkAccessOn_pm', 'WalkTransferOff_pm', 'WalkTransferOn_pm',\n",
    "                      'DirectTransferOff_nt', 'DirectTransferOn_nt', 'DriveAccessOn_nt',\n",
    "                      'EgressOff_nt', 'Off_nt', 'On_nt', 'WalkAccessOn_nt',\n",
    "                      'WalkTransferOff_nt', 'WalkTransferOn_nt' ]\n",
    "final_df = final_df[final_column_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cc88e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug_trace_flag:\n",
    "    final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96a4cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fully-qualified name of output CSV file.\n",
    "#\n",
    "# First, pluck name of scenario from last element of scenario directory name remove trailing '/'\n",
    "temp1 = scenario[0:len(scenario)-1]\n",
    "temp2 = os.path.split(temp1)\n",
    "# Get 'raw' scenario name: may have blanks. Ugh!\n",
    "raw_scenario_name = temp2[1]\n",
    "clean_scenario_name = raw_scenario_name.replace(' ', '_')\n",
    "#\n",
    "fq_output_csv_fn = sandbox_dir + clean_scenario_name + '_' + output_csv_fn_suffix + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa712847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write output CSV report file\n",
    "#\n",
    "final_df.to_csv(fq_output_csv_fn, index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-radiation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-modx_proto1_1]",
   "language": "python",
   "name": "conda-env-.conda-modx_proto1_1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
