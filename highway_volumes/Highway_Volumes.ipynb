{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccd486c7",
   "metadata": {},
   "source": [
    "# Highway Volumes Reports\n",
    "\n",
    "- Highway volumes (single report or comparison between 2 scenarios)\n",
    "    - standard\n",
    "        - aggregate (MPO, sub-region, land use classification) \n",
    "        - segmented by facility type \n",
    "    - Detailed\n",
    "        - standard report for user-select links (allow for more than 1 selection set (1,2,3,4..))\n",
    "\n",
    "- Data\n",
    "    - highway assignment results\n",
    "        - SOV, HOV assignment: Out\\**_MMA_LinkFlow.bin\n",
    "        - Truck assignment: Out\\**_MMA_LinkFlow_Trucks.bin\n",
    "            - sum the truck and SOV and HOV\n",
    "            - (look at Tot_Flow, AB_Flow, BA_Flow)\n",
    "\n",
    "\n",
    "** = AM, MD, PM, NT\n",
    "\n",
    "??? = DAT, DET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593c4071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openmatrix as omx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh\n",
    "import xarray as xr\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "import cartopy.crs as ccrs\n",
    "import csv\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dba5c9",
   "metadata": {},
   "source": [
    "Please edit config.py before using this notebook!\n",
    "\n",
    "Note: set comparison_scenario_dir to '' in the config file if not using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa9c100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===>>>USER INPUT REQUIRED: <<<===\n",
    "\n",
    "#update to your file path to confix.py\n",
    "%run \"M:/JupyterHome/JupyterNotebooks/MoDX/config.py\"\n",
    "\n",
    "## Name of file containing detailed links (if using) in base_scenario_dir and comparison_scenario_dir (if have comp scenario)\n",
    "#IF NOT USING - SET BOTH TO ''\n",
    "hwy_links_file = ''#reference_data_dir + 'sample_model_links/highway info.csv'\n",
    "ID_FIELD = ''#'TC Link ID'\n",
    "\n",
    "# END USER INPUT #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da80512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory for MoDX output for \"base year\" model results. (From config.py)\n",
    "#\n",
    "print(base_scenario_dir) #Original: r'G:/Regional_Modeling/1A_Archives/LRTP_2018/2016 Scen 00_08March2019_MoDXoutputs/'\n",
    "#\n",
    "# Base directory for MoDX output for \"comparison scenario\" model results. (Optional) (From config.py)\n",
    "# \n",
    "print(comparison_scenario_dir) #Original: r'G:/Regional_Modeling/1A_Archives/LRTP_2018/2040 NB Scen 01_MoDXoutputs/'\n",
    "\n",
    "#bring in TAZs for later use (and so don't have to repeat)\n",
    "taz = reference_data_dir + \"canonical_taz_shapefile/candidate_CTPS_TAZ_STATEWIDE_2019.shp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45ed00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up all the directories that are needed and start bringing in data\n",
    "def dataSetup(base):\n",
    "    #IMPORT links and taz spatial and attribute DATA\n",
    "    links = base + \"Databases/Statewide_Links_2018.shp\" #this will actually be in the scen directory\n",
    "\n",
    "    #load into a geodataframe\n",
    "    links_gdf = gp.read_file(links)\n",
    "    taz_gdf = gp.read_file(taz)\n",
    "    taz_gdf=taz_gdf.loc[taz_gdf['type']=='I'] #restrict just to internal tazs\n",
    "\n",
    "    #make sure crs are correct and match\n",
    "    links_gdf=links_gdf.to_crs('EPSG:26986')\n",
    "    taz_gdf=taz_gdf.to_crs('EPSG:26986')\n",
    "\n",
    "    #do attribute join to find out TAZ details for the taz each link is in\n",
    "    links_taz_gdf = links_gdf.merge(taz_gdf.loc[:, ~taz_gdf.columns.isin(['geometry', 'OBJECTID', 'Shape_Leng', 'Shape_Area'])], \n",
    "                                    left_on='TAZ_ID', right_on='taz') #exclude all columns listed above\n",
    "    #filter to links that are in internal tazs and that have taz information at all\n",
    "    highway_links_df = links_taz_gdf.loc[(links_taz_gdf['taz'] >0) & (links_taz_gdf['type']=='I') & \n",
    "                                         (links_taz_gdf['in_brmpo'] ==1)]\n",
    "    #rename ID to ID1 to make everything else work later in script\n",
    "    highway_links_df=highway_links_df.rename(columns={'ID': 'ID1'}) #won't fail if the column doesn't exist\n",
    "    #if detailed, filter\n",
    "    if len(hwy_links_file) > 4:\n",
    "        hwy_links_detail = pd.read_csv(hwy_links_file) #read in detail file\n",
    "        highway_links_list = hwy_links_detail[ID_FIELD].tolist() #grab just the ID field\n",
    "        \n",
    "        highway_links_df = highway_links_df.loc[highway_links_df['ID1'].isin(highway_links_list)]\n",
    "    return highway_links_df, links_taz_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e0d7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hwyVolSpeVOC(scen, hwy_links_file):\n",
    "        # Individual link-flow CSV tables:\n",
    "    # For each time period, there is a separate flow CSV for autos and for trucks.\n",
    "    # To get the total volume for any given time period, 'Tot_Flow' columns these need to be summed.\n",
    "    # However, the V/C and speed data for *both* autos and trucks are reported in the CSV for autos.\n",
    "    # Clear?\n",
    "    \n",
    "    home_dir = scen #whichever scenario\n",
    "\n",
    "    # Directory containing link flow CSVs\n",
    "    link_flow_dir = home_dir + 'out/'\n",
    "    #\n",
    "    am_flow_auto_fn = link_flow_dir + 'AM_MMA_LinkFlow.csv'\n",
    "    am_flow_truck_fn = link_flow_dir + 'AM_MMA_LinkFlow_Trucks.csv'\n",
    "    #\n",
    "    md_flow_auto_fn = link_flow_dir + 'MD_MMA_LinkFlow.csv'\n",
    "    md_flow_truck_fn = link_flow_dir + 'MD_MMA_LinkFlow_Trucks.csv'\n",
    "    #\n",
    "    pm_flow_auto_fn = link_flow_dir + 'PM_MMA_LinkFlow.csv'\n",
    "    pm_flow_truck_fn = link_flow_dir + 'PM_MMA_LinkFlow_Trucks.csv'\n",
    "    #\n",
    "    nt_flow_auto_fn = link_flow_dir + 'NT_MMA_LinkFlow.csv'\n",
    "    nt_flow_truck_fn = link_flow_dir + 'NT_MMA_LinkFlow_Trucks.csv'\n",
    "\n",
    "    # Read each of the above CSV files containing flow data into a dataframe\n",
    "    #\n",
    "    am_auto_df = pd.read_csv(am_flow_auto_fn, delimiter=',')\n",
    "    am_truck_df = pd.read_csv(am_flow_truck_fn, delimiter=',')\n",
    "    #\n",
    "    md_auto_df = pd.read_csv(md_flow_auto_fn, delimiter=',')\n",
    "    md_truck_df = pd.read_csv(md_flow_truck_fn, delimiter=',')\n",
    "    #\n",
    "    pm_auto_df = pd.read_csv(pm_flow_auto_fn, delimiter=',')\n",
    "    pm_truck_df = pd.read_csv(pm_flow_truck_fn, delimiter=',')\n",
    "    #\n",
    "    nt_auto_df = pd.read_csv(nt_flow_auto_fn, delimiter=',')\n",
    "    nt_truck_df = pd.read_csv(nt_flow_truck_fn, delimiter=',') \n",
    "    \n",
    "        #\n",
    "        # NOTE: volume/capacity and speed data will be harvested from the \"auto\" dataframes subsequently.\n",
    "        #       See below.\n",
    "\n",
    "    # Filter the \"flow\" datafames to only include the columns containing 'Tot_Flow' and 'Tot_VMT'(and 'ID1')\n",
    "    # \n",
    "    am_auto_vol_df = am_auto_df[['ID1', 'Tot_Flow', 'Tot_VMT']]\n",
    "    am_truck_vol_df = am_truck_df[['ID1', 'Tot_Flow', 'Tot_VMT']]\n",
    "    #\n",
    "    md_auto_vol_df = md_auto_df[['ID1', 'Tot_Flow', 'Tot_VMT']]\n",
    "    md_truck_vol_df = md_truck_df[['ID1', 'Tot_Flow', 'Tot_VMT']]\n",
    "    #\n",
    "    pm_auto_vol_df = pm_auto_df[['ID1', 'Tot_Flow', 'Tot_VMT']]\n",
    "    pm_truck_vol_df = pm_truck_df[['ID1', 'Tot_Flow', 'Tot_VMT']]\n",
    "    #\n",
    "    nt_auto_vol_df = nt_auto_df[['ID1', 'Tot_Flow', 'Tot_VMT']]\n",
    "    nt_truck_vol_df = nt_truck_df[['ID1', 'Tot_Flow', 'Tot_VMT']]\n",
    "    \n",
    "    # Rename the 'Tot_Flow' column of each dataframe, appropriately\n",
    "    #\n",
    "    am_auto_vol_df = am_auto_vol_df.rename(columns={'Tot_Flow' : 'Tot_Flow_am_auto', 'Tot_VMT' : 'Tot_VMT_am_auto'})\n",
    "    am_truck_vol_df = am_truck_vol_df.rename(columns={'Tot_Flow' : 'Tot_Flow_am_truck', 'Tot_VMT' : 'Tot_VMT_am_truck'})\n",
    "    #\n",
    "    md_auto_vol_df = md_auto_vol_df.rename(columns={'Tot_Flow' : 'Tot_Flow_md_auto', 'Tot_VMT' : 'Tot_VMT_md_auto'})\n",
    "    md_truck_vol_df = md_truck_vol_df.rename(columns={'Tot_Flow' : 'Tot_Flow_md_truck', 'Tot_VMT' : 'Tot_VMT_md_truck'})\n",
    "    #\n",
    "    pm_auto_vol_df = pm_auto_vol_df.rename(columns={'Tot_Flow' : 'Tot_Flow_pm_auto', 'Tot_VMT' : 'Tot_VMT_pm_auto'})\n",
    "    pm_truck_vol_df = pm_truck_vol_df.rename(columns={'Tot_Flow' : 'Tot_Flow_pm_truck', 'Tot_VMT' : 'Tot_VMT_pm_truck'})\n",
    "    #\n",
    "    nt_auto_vol_df = nt_auto_vol_df.rename(columns={'Tot_Flow' : 'Tot_Flow_nt_auto', 'Tot_VMT' : 'Tot_VMT_nt_auto'})\n",
    "    nt_truck_vol_df = nt_truck_vol_df.rename(columns={'Tot_Flow' : 'Tot_Flow_nt_truck', 'Tot_VMT' : 'Tot_VMT_nt_truck'})\n",
    "\n",
    "\n",
    "    # Index all the \"volume\" dataframes on \"ID1\", in preparation for joining\n",
    "    #\n",
    "    am_auto_vol_df.set_index(\"ID1\")\n",
    "    am_truck_vol_df.set_index(\"ID1\")\n",
    "    #\n",
    "    md_auto_vol_df.set_index(\"ID1\")\n",
    "    md_truck_vol_df.set_index(\"ID1\")\n",
    "    #\n",
    "    pm_auto_vol_df.set_index(\"ID1\")\n",
    "    pm_truck_vol_df.set_index(\"ID1\")\n",
    "    #\n",
    "    nt_auto_vol_df.set_index(\"ID1\")\n",
    "    nt_truck_vol_df.set_index(\"ID1\")\n",
    "\n",
    "    # Join the \"volume\" dataframes\n",
    "    j1_df = am_auto_vol_df.join(am_truck_vol_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "    #\n",
    "    j1_df.set_index(\"ID1\")\n",
    "    j2_df = j1_df.join(md_auto_vol_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "    #\n",
    "    j2_df.set_index(\"ID1\")\n",
    "    j3_df = j2_df.join(md_truck_vol_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "    #\n",
    "    j3_df.set_index(\"ID1\")\n",
    "    j4_df = j3_df.join(pm_auto_vol_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "    #\n",
    "    j4_df.set_index(\"ID1\")\n",
    "    j5_df = j4_df.join(pm_truck_vol_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "    #\n",
    "    j5_df.set_index(\"ID1\")\n",
    "    j6_df = j5_df.join(nt_auto_vol_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "    #\n",
    "    j6_df.set_index(\"ID1\")\n",
    "    total_flow_join = j6_df.join(nt_truck_vol_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "    \n",
    "    # Calculate the total volume (auto + truck) for each time period, and for the entire day\n",
    "    #\n",
    "    total_flow_join['Tot_Flow_am'] = total_flow_join['Tot_Flow_am_auto'] + total_flow_join['Tot_Flow_am_truck']\n",
    "    #\n",
    "    total_flow_join['Tot_Flow_md'] = total_flow_join['Tot_Flow_md_auto'] + total_flow_join['Tot_Flow_md_truck']\n",
    "    #\n",
    "    total_flow_join['Tot_Flow_pm'] = total_flow_join['Tot_Flow_pm_auto'] + total_flow_join['Tot_Flow_pm_truck']\n",
    "    #\n",
    "    total_flow_join['Tot_Flow_nt'] = total_flow_join['Tot_Flow_nt_auto'] + total_flow_join['Tot_Flow_nt_truck']\n",
    "    #\n",
    "    total_flow_join['Tot_Flow_daily'] = (total_flow_join['Tot_Flow_am'] + total_flow_join['Tot_Flow_md'] + \n",
    "                                         total_flow_join['Tot_Flow_pm'] + total_flow_join['Tot_Flow_nt'])\n",
    "    \n",
    "    # Calculate the total volume (auto + truck) for each time period, and for the entire day (VMT STYLE!)\n",
    "    #\n",
    "    total_flow_join['Tot_VMT_am'] = total_flow_join['Tot_VMT_am_auto'] + total_flow_join['Tot_VMT_am_truck']\n",
    "    #\n",
    "    total_flow_join['Tot_VMT_md'] = total_flow_join['Tot_VMT_md_auto'] + total_flow_join['Tot_VMT_md_truck']\n",
    "    #\n",
    "    total_flow_join['Tot_VMT_pm'] = total_flow_join['Tot_VMT_pm_auto'] + total_flow_join['Tot_VMT_pm_truck']\n",
    "    #\n",
    "    total_flow_join['Tot_VMT_nt'] = total_flow_join['Tot_VMT_nt_auto'] + total_flow_join['Tot_VMT_nt_truck']\n",
    "    #\n",
    "    total_flow_join['Tot_VMT_daily'] = (total_flow_join['Tot_VMT_am'] + total_flow_join['Tot_VMT_md'] + \n",
    "                                         total_flow_join['Tot_VMT_pm'] + total_flow_join['Tot_VMT_nt'])\n",
    "\n",
    "    #set index\n",
    "    total_flow_join.set_index(\"ID1\")\n",
    "\n",
    "\n",
    "    # Harvest the speed and volume-to-capacity ratio data from the 4 \"auto\" dataframes, one for each time period. (See above.)\n",
    "    # Note Python variable naming convention used here: \"svc\" == \"speed and volume/capacity\"\n",
    "    #\n",
    "    am_svc_df = am_auto_df[['ID1', 'AB_Speed', 'BA_Speed', 'AB_VOC', 'BA_VOC']]\n",
    "    #\n",
    "    md_svc_df = md_auto_df[['ID1', 'AB_Speed', 'BA_Speed', 'AB_VOC', 'BA_VOC']]\n",
    "    #\n",
    "    pm_svc_df = pm_auto_df[['ID1', 'AB_Speed', 'BA_Speed', 'AB_VOC', 'BA_VOC']]\n",
    "    #\n",
    "    nt_svc_df = nt_auto_df[['ID1', 'AB_Speed', 'BA_Speed', 'AB_VOC', 'BA_VOC']]\n",
    "\n",
    "    # Rename the columns of these \"svc\" dataframes in preparation for joining them with the speed dataframe, computed above.\n",
    "    #\n",
    "    am_svc_df = am_svc_df.rename(columns={'AB_Speed' : 'AB_Speed_am', \n",
    "                                          'BA_Speed' : 'BA_Speed_am',\n",
    "                                          'AB_VOC'   : 'AB_VOC_am', \n",
    "                                          'BA_VOC'   : 'BA_VOC_am'})\n",
    "    #\n",
    "    md_svc_df = md_svc_df.rename(columns={'AB_Speed' : 'AB_Speed_md', \n",
    "                                          'BA_Speed' : 'BA_Speed_md',\n",
    "                                          'AB_VOC'   : 'AB_VOC_md', \n",
    "                                          'BA_VOC'   : 'BA_VOC_md'})\n",
    "    #\n",
    "    pm_svc_df = pm_svc_df.rename(columns={'AB_Speed' : 'AB_Speed_pm', \n",
    "                                          'BA_Speed' : 'BA_Speed_pm',\n",
    "                                          'AB_VOC'   : 'AB_VOC_pm', \n",
    "                                          'BA_VOC'   : 'BA_VOC_pm'})\n",
    "    #\n",
    "    nt_svc_df = nt_svc_df.rename(columns={'AB_Speed' : 'AB_Speed_nt', \n",
    "                                          'BA_Speed' : 'BA_Speed_nt',\n",
    "                                          'AB_VOC'   : 'AB_VOC_nt', \n",
    "                                          'BA_VOC'   : 'BA_VOC_nt'})\n",
    "    \n",
    "    # Per instructions from Marty on June 22, 2021:\n",
    "    # For a given time period, calculate the MIN of the AB_Speed and BA_Speed, and the MAX of the AB_VOC and BA_VOC.\n",
    "    # Basically, the idea is to flag the link direction with the most congestion.\n",
    "    #\n",
    "    am_svc_df['Speed_am'] = am_svc_df.apply(lambda x: min(x['AB_Speed_am'], x['BA_Speed_am']), axis=1)\n",
    "    am_svc_df['VOC_am'] = am_svc_df.apply(lambda x: max(x['AB_VOC_am'], x['BA_VOC_am']), axis=1)\n",
    "    #\n",
    "    md_svc_df['Speed_md'] = md_svc_df.apply(lambda x: min(x['AB_Speed_md'], x['BA_Speed_md']), axis=1)\n",
    "    md_svc_df['VOC_md'] = md_svc_df.apply(lambda x: max(x['AB_VOC_md'], x['BA_VOC_md']), axis=1)\n",
    "    #\n",
    "    pm_svc_df['Speed_pm'] = pm_svc_df.apply(lambda x: min(x['AB_Speed_pm'], x['BA_Speed_pm']), axis=1)\n",
    "    pm_svc_df['VOC_pm'] = pm_svc_df.apply(lambda x: max(x['AB_VOC_pm'], x['BA_VOC_pm']), axis=1)\n",
    "    #\n",
    "    nt_svc_df['Speed_nt'] = nt_svc_df.apply(lambda x: min(x['AB_Speed_nt'], x['BA_Speed_nt']), axis=1)\n",
    "    nt_svc_df['VOC_nt'] = nt_svc_df.apply(lambda x: max(x['AB_VOC_nt'], x['BA_VOC_nt']), axis=1)\n",
    "\n",
    "    # Index the \"svc\" dataframes in preparation for joining\n",
    "    am_svc_df.set_index(\"ID1\")\n",
    "    md_svc_df.set_index(\"ID1\")\n",
    "    pm_svc_df.set_index(\"ID1\")\n",
    "    nt_svc_df.set_index(\"ID1\")\n",
    "\n",
    "    # Join the speed and volume/capacity data to the volume data collected above into a single dataframe.\n",
    "    #\n",
    "    j7_df = total_flow_join.join(am_svc_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "    j7_df.set_index(\"ID1\")\n",
    "    #\n",
    "    j8_df = j7_df.join(md_svc_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "    j8_df.set_index(\"ID1\")\n",
    "    #\n",
    "    j9_df = j8_df.join(pm_svc_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "    j9_df.set_index(\"ID1\")\n",
    "    #\n",
    "    all_data_df = j9_df.join(nt_svc_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "    \n",
    "    return all_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929dc07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hwyGraphs(all_data_df, scen):\n",
    "#restrict to just volume (vmt) data for standard graphs and aggregation numbers\n",
    "    links4graph=highway_links_df.merge(all_data_df[['ID1','Tot_VMT_am', 'Tot_VMT_md',\n",
    "                                            'Tot_VMT_pm','Tot_VMT_nt','Tot_VMT_daily']],\n",
    "                               how = 'left', left_on ='ID1', right_on='ID1')\n",
    "\n",
    "    if len(hwy_links_file) < 4: #STANDARD\n",
    "            #make graphs\n",
    "        #first aggregate to functional class\n",
    "        volFU = links4graph.groupby(['SCEN_00_FU'])[['Tot_VMT_am', 'Tot_VMT_md','Tot_VMT_pm','Tot_VMT_nt','Tot_VMT_daily']].agg('sum')\n",
    "        volFU = volFU.reset_index()\n",
    "\n",
    "        #make func class into categories\n",
    "        volFU['Functional Class'] = np.where(volFU['SCEN_00_FU'].isin([72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84]), 'Ramps',\n",
    "                                            np.where(volFU['SCEN_00_FU'].isin([50,51,52,53,54,55,56]), 'HOV', \n",
    "                                                    np.where(volFU['SCEN_00_FU'].isin([1,2,3,5,6,10]), volFU['SCEN_00_FU'], 'Other')))\n",
    "        volFU['Functional Class'] = volFU['Functional Class'].apply(str)\n",
    "        volFU = volFU.groupby(['Functional Class'])[['Tot_VMT_am', 'Tot_VMT_md','Tot_VMT_pm','Tot_VMT_nt','Tot_VMT_daily']].agg('sum')\n",
    "        volFU = volFU.reset_index()\n",
    "\n",
    "        volFUgraph = px.bar(volFU, \n",
    "                        x = 'Functional Class', y = ['Tot_VMT_am','Tot_VMT_md','Tot_VMT_pm','Tot_VMT_nt'], \n",
    "                        title=scen +' Volumes by Functional Class/Facility Type', labels = {'value':'VMT'},\n",
    "                            hover_data=['Tot_VMT_daily'])\n",
    "    else: #DETAILED\n",
    "        volFU =links4graph #no group by because individual links\n",
    "        volFU['ID1'] = volFU['ID1'].astype('str')\n",
    "        volFUgraph = px.bar(volFU, \n",
    "                        x = ['Tot_VMT_am','Tot_VMT_md','Tot_VMT_pm','Tot_VMT_nt'], y = 'ID1', \n",
    "                        title=scen +' Volumes for Selected Link IDs', height = 1000, \n",
    "                        labels = {'ID1': 'Link ID', 'value':'VMT'}, orientation ='h', hover_data=['Tot_VMT_daily'])\n",
    "        volFUgraph.update_yaxes(type='category')\n",
    "        \n",
    "    return volFUgraph, links4graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d13c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeNums(all_data_df, links4graph):\n",
    "    #get aggregation numbers\n",
    "    if len(hwy_links_file) < 4:\n",
    "        #filter for aggregation numbers\n",
    "        links4num = links4graph.loc[links4graph['in_brmpo']==1]\n",
    "        #volume sums for the MPO\n",
    "        volMPO=links4num.groupby(['in_brmpo'])[['Tot_VMT_am', 'Tot_VMT_md','Tot_VMT_pm','Tot_VMT_nt','Tot_VMT_daily']].agg('sum')\n",
    "\n",
    "        #volume sums for the MPO subregions\n",
    "        volSubReg=links4num.groupby(['subregion'])[['Tot_VMT_am', 'Tot_VMT_md','Tot_VMT_pm','Tot_VMT_nt','Tot_VMT_daily']].agg('sum')\n",
    "\n",
    "        #volume sums for all the TAZs\n",
    "        volTAZ=links4num.groupby(['taz'])[['Tot_VMT_am', 'Tot_VMT_md','Tot_VMT_pm','Tot_VMT_nt','Tot_VMT_daily']].agg('sum')\n",
    "    else:\n",
    "        volMPO = 'Please run STANDARD version to get VMT at these geographies'\n",
    "        volSubReg = 'Please run STANDARD version to get VMT at these geographies'\n",
    "        volTAZ = 'Please run STANDARD version to get VMT at these geographies'\n",
    "    \n",
    "    return volMPO, volSubReg, volTAZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fce542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate difference graphs and tables\n",
    "def calcSum(outputs):\n",
    "    #get difference between scenarios at a link or functional class level\n",
    "    dif = outputs['Base Scenario'][0]-outputs['Comparison Scenario'][0]\n",
    "    if len(hwy_links_file) < 4:\n",
    "        #get the difference between the key summary outputs only if STANDARD\n",
    "        #do not include geometry field - empty and has issues for difference.\n",
    "        volMPO1 = pd.DataFrame(outputs['Base Scenario'][2])-pd.DataFrame(outputs['Comparison Scenario'][2])\n",
    "        volSubReg1 = pd.DataFrame(outputs['Base Scenario'][3])-pd.DataFrame(outputs['Comparison Scenario'][3])\n",
    "        volTAZ1 = pd.DataFrame(outputs['Base Scenario'][4])-pd.DataFrame(outputs['Comparison Scenario'][4])\n",
    "        \n",
    "        #create summary tables (already exist if just base)\n",
    "        #two steps to each:\n",
    "        #1. SubRegions of MPO\n",
    "        volSubReg = pd.DataFrame(outputs['Base Scenario'][3]).merge(pd.DataFrame(outputs['Comparison Scenario'][3]), how = 'outer', on='subregion', suffixes = ('_base', '_comp'))\n",
    "        volSubReg1 =volSubReg1.add_suffix('_dif') #differentiate columns for diff\n",
    "        volSubReg = volSubReg.merge(volSubReg1, how = 'outer',on='subregion')\n",
    "        #2 TAZs of MPO\n",
    "        volTAZ= pd.DataFrame(outputs['Base Scenario'][4]).merge(pd.DataFrame(outputs['Comparison Scenario'][4]), how = 'outer',on='taz', suffixes = ('_base', '_comp'))\n",
    "        volTAZ1 =volTAZ1.add_suffix('_dif') #differentiate columns for diff\n",
    "        volTAZ= volTAZ.merge(pd.DataFrame(volTAZ1), how = 'outer',on='taz')\n",
    "        ##3. MPO Aggregation (just one row so append)\n",
    "            #first update index\n",
    "        outputs['Base Scenario'][2]['Scenario']='Base'\n",
    "        outputs['Comparison Scenario'][2]['Scenario']='Comparison'\n",
    "        volMPO1['Scenario']='Difference'\n",
    "            #now actually do the table\n",
    "        volMPO = pd.DataFrame(outputs['Base Scenario'][2]).append(pd.DataFrame(outputs['Comparison Scenario'][2]))\n",
    "        volMPO = volMPO.append(pd.DataFrame(volMPO1))\n",
    "        \n",
    "    else: #IF DETAILED (aka links selected)\n",
    "        volMPO = 'Please run STANDARD version to get VMT at these geographies'\n",
    "        volSubReg = 'Please run STANDARD version to get VMT at these geographies'\n",
    "        volTAZ = 'Please run STANDARD version to get VMT at these geographies'\n",
    "            \n",
    "    #create a difference graph\n",
    "    graph, links4graph = hwyGraphs(dif, 'Difference')\n",
    "    #save results into dictionary\n",
    "    outputs['Difference'] = [dif, graph, volMPO, volSubReg, volTAZ]\n",
    "    \n",
    "    return outputs, volMPO, volSubReg, volTAZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a218ada",
   "metadata": {},
   "source": [
    "## Run All Functions (Create the Report Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256dcd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only use second scenario if provided\n",
    "if len(comparison_scenario_dir) > 0:\n",
    "    scenlist =[base_scenario_dir, comparison_scenario_dir]\n",
    "else:\n",
    "    scenlist = [base_scenario_dir]\n",
    "\n",
    "#create output dictionary\n",
    "outputs = {}\n",
    "for scen in scenlist:\n",
    "    #import and clean links and tazs data tables for the scenario\n",
    "    highway_links_df, links_taz_gdf = dataSetup(scen)\n",
    "    #join a bunch of speed, volume, flow, and calculate VOC\n",
    "    all_data_df = hwyVolSpeVOC(scen, hwy_links_file)\n",
    "    #title of scenario is important for graphing in the title\n",
    "    if scen == scenlist[0]:\n",
    "        s = 'Base Scenario'\n",
    "    else:\n",
    "        s = 'Comparison Scenario'\n",
    "    graph, links4graph = hwyGraphs(all_data_df, s) #make the graph!\n",
    "    volMPO, volSubReg, volTAZ = makeNums(all_data_df, links4graph) #make the summary numbers!\n",
    "    outputs[s] = [all_data_df, graph, volMPO, volSubReg, volTAZ] #save it all!\n",
    "#only do difference if two scenarios:\n",
    "if len(scenlist) > 1:\n",
    "    #calculate the summary tables and add difference to everything\n",
    "    outputs, volMPO, volSubReg, volTAZ = calcSum(outputs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de285a64",
   "metadata": {},
   "source": [
    "## See Resulting Graph(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7639f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['Base Scenario'][1].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3154359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL - ONLY RUN IF COMPARISON SCENARIO\n",
    "try:\n",
    "    outputs['Comparison Scenario'][1].show()\n",
    "except KeyError:\n",
    "    print('No Comparison Scenario Input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18721939",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL - ONLY RUN IF COMPARISON SCENARIO\n",
    "try:\n",
    "    outputs['Difference'][1].show()\n",
    "except KeyError:\n",
    "    print('No Comparison Scenario Input')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c764712",
   "metadata": {},
   "source": [
    "## Show Summary Numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e24e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MPO\n",
    "volMPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68a8572",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SubRegions\n",
    "volSubReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd0c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TAZ\n",
    "volTAZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-plasma",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:modx_proto1] *",
   "language": "python",
   "name": "conda-env-modx_proto1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
