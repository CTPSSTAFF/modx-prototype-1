{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "downtown-ethnic",
   "metadata": {},
   "source": [
    "## Generate report (flow, speed, and V/C) for links with a given functional class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highway links report (flow, V/C, and speeds) by functional class\n",
    "#\n",
    "import openmatrix as omx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory for MoDX output for \"base year\" model results.\n",
    "#\n",
    "base_scenario_dir = r'G:/Regional_Modeling/1A_Archives/LRTP_2018/2016 Scen 00_08March2019_MoDXoutputs/'\n",
    "#\n",
    "# Base directory for MoDX output for \"comparison scenario\" model results.\n",
    "# \n",
    "comparison_scenario_dir = r'G:/Regional_Modeling/1A_Archives/LRTP_2018/2040 NB Scen 01_MoDXoutputs/'\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-validity",
   "metadata": {},
   "source": [
    "### User input required: Path to root directory for current scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===>>>USER INPUT REQUIRED: <<<===\n",
    "#\n",
    "# 1. Supply path to root directory of scenario to use for the current run of this notebook:\n",
    "# \n",
    "home_dir = base_scenario_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-offset",
   "metadata": {},
   "source": [
    "### User input required: Path to root of user's \"sandbox\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-airplane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Supply path to root of user's \"sandbox\" directory:\n",
    "#\n",
    "my_sandbox_dir = r'S:/my_modx_output_dir/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-union",
   "metadata": {},
   "source": [
    "### User input required: CSV file for report output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Supply name of CSV output file for tabular results generated by this notebook:\n",
    "#\n",
    "csv_output_fn = 'functional_class_report_base_scenario.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-haven",
   "metadata": {},
   "source": [
    "### User input required: Python list of functional class number(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-senator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Supply array of selected functional class number(s).\n",
    "#    NOTE: Even if a report for a _single_ functionl class is desired, this value must be supplied in a list!\n",
    "#\n",
    "selected_fc_list = [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory in which the spatial data for the model network links is stored (both shapefile and GeoJSON formats)\n",
    "links_spatial_data_dir = r'G:/Data_Resources/modx/statewide_links_shapefile/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the links shapefile into a geopandas dataframe \n",
    "links_shapefile_fn = 'Statewide_Links_2018_BK_EPSG26986.shp'\n",
    "fq_links_shapefile_fn = links_spatial_data_dir + links_shapefile_fn\n",
    "links_gdf = gp.read_file(fq_links_shapefile_fn)\n",
    "links_gdf.set_index(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-treasurer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of links with the specified functional clas(ses).\n",
    "# NOTE: The attribute to query is 'SCEN_00_FU', not the most felicitous choice of attribute name!\n",
    "#\n",
    "filtered_gdf = links_gdf[links_gdf['SCEN_00_FU'].isin(selected_fc_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of IDs for the links with the specified functional clas(ses).\n",
    "highway_links_list = filtered_gdf['ID'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-prize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing link flow CSVs\n",
    "link_flow_dir = home_dir + 'out/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual link-flow CSV tables:\n",
    "# For each time period, there is a separate flow CSV for autos and for trucks.\n",
    "# To get the total volume for any given time period, 'Tot_Flow' columns these need to be summed.\n",
    "# However, the V/C and speed data for *both* autos and trucks are reported in the CSV for autos.\n",
    "# Clear?\n",
    "#\n",
    "am_flow_auto_fn = link_flow_dir + 'AM_MMA_LinkFlow.csv'\n",
    "am_flow_truck_fn = link_flow_dir + 'AM_MMA_LinkFlow_Trucks.csv'\n",
    "#\n",
    "md_flow_auto_fn = link_flow_dir + 'MD_MMA_LinkFlow.csv'\n",
    "md_flow_truck_fn = link_flow_dir + 'MD_MMA_LinkFlow_Trucks.csv'\n",
    "#\n",
    "pm_flow_auto_fn = link_flow_dir + 'PM_MMA_LinkFlow.csv'\n",
    "pm_flow_truck_fn = link_flow_dir + 'PM_MMA_LinkFlow_Trucks.csv'\n",
    "#\n",
    "nt_flow_auto_fn = link_flow_dir + 'NT_MMA_LinkFlow.csv'\n",
    "nt_flow_truck_fn = link_flow_dir + 'NT_MMA_LinkFlow_Trucks.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read each of the above CSV files containing flow data into a dataframe\n",
    "#\n",
    "temp_am_auto_df = pd.read_csv(am_flow_auto_fn, delimiter=',')\n",
    "temp_am_truck_df = pd.read_csv(am_flow_truck_fn, delimiter=',')\n",
    "#\n",
    "temp_md_auto_df = pd.read_csv(md_flow_auto_fn, delimiter=',')\n",
    "temp_md_truck_df = pd.read_csv(md_flow_truck_fn, delimiter=',')\n",
    "#\n",
    "temp_pm_auto_df = pd.read_csv(pm_flow_auto_fn, delimiter=',')\n",
    "temp_pm_truck_df = pd.read_csv(pm_flow_truck_fn, delimiter=',')\n",
    "#\n",
    "temp_nt_auto_df = pd.read_csv(nt_flow_auto_fn, delimiter=',')\n",
    "temp_nt_truck_df = pd.read_csv(nt_flow_truck_fn, delimiter=',') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the 8 temp \"flow\" dataframes to only include rows for the selected highway links\n",
    "#\n",
    "am_auto_df = temp_am_auto_df[temp_am_auto_df['ID1'].isin(highway_links_list)]\n",
    "am_truck_df = temp_am_truck_df[temp_am_truck_df['ID1'].isin(highway_links_list)]\n",
    "#\n",
    "md_auto_df = temp_md_auto_df[temp_md_auto_df['ID1'].isin(highway_links_list)]\n",
    "md_truck_df = temp_md_truck_df[temp_md_truck_df['ID1'].isin(highway_links_list)]\n",
    "#\n",
    "pm_auto_df = temp_pm_auto_df[temp_pm_auto_df['ID1'].isin(highway_links_list)]\n",
    "pm_truck_df = temp_pm_truck_df[temp_pm_truck_df['ID1'].isin(highway_links_list)]\n",
    "#\n",
    "nt_auto_df = temp_nt_auto_df[temp_nt_auto_df['ID1'].isin(highway_links_list)]\n",
    "nt_truck_df = temp_nt_truck_df[temp_nt_truck_df['ID1'].isin(highway_links_list)]\n",
    "#\n",
    "# NOTE: volume/capacity and speed data will be harvested from the \"auto\" dataframes subsequently.\n",
    "#       See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further filter the filetered \"flow\" datafames to only include the columns containing 'Tot_Flow' (and 'ID1' and 'SCEN_00_FU')\n",
    "# \n",
    "am_auto_vol_df = am_auto_df[['ID1', 'Tot_Flow']]\n",
    "am_truck_vol_df = am_truck_df[['ID1', 'Tot_Flow']]\n",
    "#\n",
    "md_auto_vol_df = md_auto_df[['ID1', 'Tot_Flow']]\n",
    "md_truck_vol_df = md_truck_df[['ID1', 'Tot_Flow']]\n",
    "#\n",
    "pm_auto_vol_df = pm_auto_df[['ID1', 'Tot_Flow']]\n",
    "pm_truck_vol_df = pm_truck_df[['ID1', 'Tot_Flow']]\n",
    "#\n",
    "nt_auto_vol_df = nt_auto_df[['ID1', 'Tot_Flow']]\n",
    "nt_truck_vol_df = nt_truck_df[['ID1', 'Tot_Flow']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the 'Tot_Flow' column of each dataframe, appropriately\n",
    "#\n",
    "am_auto_vol_df = am_auto_vol_df.rename(columns={'Tot_Flow' : 'Tot_Flow_am_auto'})\n",
    "am_truck_vol_df = am_truck_vol_df.rename(columns={'Tot_Flow' : 'Tot_Flow_am_truck'})\n",
    "#\n",
    "md_auto_vol_df = md_auto_vol_df.rename(columns={'Tot_Flow' : 'Tot_Flow_md_auto'})\n",
    "md_truck_vol_df = md_truck_vol_df.rename(columns={'Tot_Flow' : 'Tot_Flow_md_truck'})\n",
    "#\n",
    "pm_auto_vol_df = pm_auto_vol_df.rename(columns={'Tot_Flow' : 'Tot_Flow_pm_auto'})\n",
    "pm_truck_vol_df = pm_truck_vol_df.rename(columns={'Tot_Flow' : 'Tot_Flow_pm_truck'})\n",
    "#\n",
    "nt_auto_vol_df = nt_auto_vol_df.rename(columns={'Tot_Flow' : 'Tot_Flow_nt_auto'})\n",
    "nt_truck_vol_df = nt_truck_vol_df.rename(columns={'Tot_Flow' : 'Tot_Flow_nt_truck'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index all the \"volume\" dataframes on \"ID1\", in preparation for joining\n",
    "#\n",
    "am_auto_vol_df.set_index(\"ID1\")\n",
    "am_truck_vol_df.set_index(\"ID1\")\n",
    "#\n",
    "md_auto_vol_df.set_index(\"ID1\")\n",
    "md_truck_vol_df.set_index(\"ID1\")\n",
    "#\n",
    "pm_auto_vol_df.set_index(\"ID1\")\n",
    "pm_truck_vol_df.set_index(\"ID1\")\n",
    "#\n",
    "nt_auto_vol_df.set_index(\"ID1\")\n",
    "nt_truck_vol_df.set_index(\"ID1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the \"volume\" dataframes\n",
    "j1_df = am_auto_vol_df.join(am_truck_vol_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "#\n",
    "j1_df.set_index(\"ID1\")\n",
    "j2_df = j1_df.join(md_auto_vol_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "#\n",
    "j2_df.set_index(\"ID1\")\n",
    "j3_df = j2_df.join(md_truck_vol_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "#\n",
    "j3_df.set_index(\"ID1\")\n",
    "j4_df = j3_df.join(pm_auto_vol_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "#\n",
    "j4_df.set_index(\"ID1\")\n",
    "j5_df = j4_df.join(pm_truck_vol_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "#\n",
    "j5_df.set_index(\"ID1\")\n",
    "j6_df = j5_df.join(nt_auto_vol_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "#\n",
    "j6_df.set_index(\"ID1\")\n",
    "total_flow_join = j6_df.join(nt_truck_vol_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "#\n",
    "total_flow_join.set_index(\"ID1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total volume (auto + truck) for each time period, and for the entire day\n",
    "#\n",
    "total_flow_join['Tot_Flow_am'] = total_flow_join['Tot_Flow_am_auto'] + total_flow_join['Tot_Flow_am_truck']\n",
    "#\n",
    "total_flow_join['Tot_Flow_md'] = total_flow_join['Tot_Flow_md_auto'] + total_flow_join['Tot_Flow_md_truck']\n",
    "#\n",
    "total_flow_join['Tot_Flow_pm'] = total_flow_join['Tot_Flow_pm_auto'] + total_flow_join['Tot_Flow_pm_truck']\n",
    "#\n",
    "total_flow_join['Tot_Flow_nt'] = total_flow_join['Tot_Flow_nt_auto'] + total_flow_join['Tot_Flow_nt_truck']\n",
    "#\n",
    "total_flow_join['Tot_Flow_daily'] = total_flow_join['Tot_Flow_am'] + total_flow_join['Tot_Flow_md'] + \\\n",
    "                                    total_flow_join['Tot_Flow_pm'] + total_flow_join['Tot_Flow_nt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "total_flow_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_flow_join.set_index(\"ID1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harvest the speed and volume-to-capacity ratio data from the 4 \"auto\" dataframes, one for each time period. (See above.)\n",
    "# Note Python variable naming convention used here: \"svc\" == \"speed and volume/capacity\"\n",
    "#\n",
    "am_svc_df = am_auto_df[['ID1', 'AB_Speed', 'BA_Speed', 'AB_VOC', 'BA_VOC']]\n",
    "#\n",
    "md_svc_df = md_auto_df[['ID1', 'AB_Speed', 'BA_Speed', 'AB_VOC', 'BA_VOC']]\n",
    "#\n",
    "pm_svc_df = pm_auto_df[['ID1', 'AB_Speed', 'BA_Speed', 'AB_VOC', 'BA_VOC']]\n",
    "#\n",
    "nt_svc_df = nt_auto_df[['ID1', 'AB_Speed', 'BA_Speed', 'AB_VOC', 'BA_VOC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-transcript",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "am_svc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns of these \"svc\" dataframes in preparation for joining them with the speed dataframe, computed above.\n",
    "#\n",
    "am_svc_df = am_svc_df.rename(columns={'AB_Speed' : 'AB_Speed_am', \n",
    "                                      'BA_Speed' : 'BA_Speed_am',\n",
    "                                      'AB_VOC'   : 'AB_VOC_am', \n",
    "                                      'BA_VOC'   : 'BA_VOC_am'})\n",
    "#\n",
    "md_svc_df = md_svc_df.rename(columns={'AB_Speed' : 'AB_Speed_md', \n",
    "                                      'BA_Speed' : 'BA_Speed_md',\n",
    "                                      'AB_VOC'   : 'AB_VOC_md', \n",
    "                                      'BA_VOC'   : 'BA_VOC_md'})\n",
    "#\n",
    "pm_svc_df = pm_svc_df.rename(columns={'AB_Speed' : 'AB_Speed_pm', \n",
    "                                      'BA_Speed' : 'BA_Speed_pm',\n",
    "                                      'AB_VOC'   : 'AB_VOC_pm', \n",
    "                                      'BA_VOC'   : 'BA_VOC_pm'})\n",
    "#\n",
    "nt_svc_df = nt_svc_df.rename(columns={'AB_Speed' : 'AB_Speed_nt', \n",
    "                                      'BA_Speed' : 'BA_Speed_nt',\n",
    "                                      'AB_VOC'   : 'AB_VOC_nt', \n",
    "                                      'BA_VOC'   : 'BA_VOC_nt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "nt_svc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per instructions from Marty on June 22, 2021:\n",
    "# For a given time period, calculate the MIN of the AB_Speed and BA_Speed, and the MAX of the AB_VOC and BA_VOC.\n",
    "# Basically, the idea is to flag the link direction with the most congestion.\n",
    "#\n",
    "am_svc_df['Speed_am'] = am_svc_df.apply(lambda x: min(x['AB_Speed_am'], x['BA_Speed_am']), axis=1)\n",
    "am_svc_df['VOC_am'] = am_svc_df.apply(lambda x: max(x['AB_VOC_am'], x['BA_VOC_am']), axis=1)\n",
    "#\n",
    "md_svc_df['Speed_md'] = md_svc_df.apply(lambda x: min(x['AB_Speed_md'], x['BA_Speed_md']), axis=1)\n",
    "md_svc_df['VOC_md'] = md_svc_df.apply(lambda x: max(x['AB_VOC_md'], x['BA_VOC_md']), axis=1)\n",
    "#\n",
    "pm_svc_df['Speed_pm'] = pm_svc_df.apply(lambda x: min(x['AB_Speed_pm'], x['BA_Speed_pm']), axis=1)\n",
    "pm_svc_df['VOC_pm'] = pm_svc_df.apply(lambda x: max(x['AB_VOC_pm'], x['BA_VOC_pm']), axis=1)\n",
    "#\n",
    "nt_svc_df['Speed_nt'] = nt_svc_df.apply(lambda x: min(x['AB_Speed_nt'], x['BA_Speed_nt']), axis=1)\n",
    "nt_svc_df['VOC_nt'] = nt_svc_df.apply(lambda x: max(x['AB_VOC_nt'], x['BA_VOC_nt']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Santiy check #1\n",
    "am_svc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Santiy check #2\n",
    "md_svc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Santiy check #3\n",
    "pm_svc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Santiy check #4\n",
    "nt_svc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index the \"svc\" dataframes in preparation for joining\n",
    "am_svc_df.set_index(\"ID1\")\n",
    "md_svc_df.set_index(\"ID1\")\n",
    "pm_svc_df.set_index(\"ID1\")\n",
    "nt_svc_df.set_index(\"ID1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the speed and volume/capacity data to the volume data collected above into a single dataframe.\n",
    "#\n",
    "j7_df = total_flow_join.join(am_svc_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "j7_df.set_index(\"ID1\")\n",
    "#\n",
    "j8_df = j7_df.join(md_svc_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "j8_df.set_index(\"ID1\")\n",
    "#\n",
    "j9_df = j8_df.join(pm_svc_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "j9_df.set_index(\"ID1\")\n",
    "#\n",
    "all_data_df = j9_df.join(nt_svc_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "all_data_df.set_index(\"ID1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-zimbabwe",
   "metadata": {},
   "source": [
    "### Export report output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-desert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the dataframe as a CSV file\n",
    "fq_output_fn = my_sandbox_dir + csv_output_fn\n",
    "all_data_df.to_csv(fq_output_fn, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-magnitude",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-modx_proto1] *",
   "language": "python",
   "name": "conda-env-.conda-modx_proto1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
