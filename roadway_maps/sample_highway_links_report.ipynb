{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "associate-custom",
   "metadata": {},
   "source": [
    "## Generate report of flow, speed, and V/C ratio for selected set of links\n",
    "\n",
    "Links are specified in an input CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample highway links flow, V/C, and speeds notebook\n",
    "#\n",
    "import openmatrix as omx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d4450",
   "metadata": {},
   "source": [
    "### User input required: Specify paths to input and output directories in config.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee7792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"C:/Users/ben_k/work_stuff/modx-prototype-1/config.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ed67eb",
   "metadata": {},
   "source": [
    "### User input required: Specify scenario to be used for the current run of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-tournament",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify scenario to be used for the current run of this notebook\n",
    "# \n",
    "home_dir =  comparison_scenario_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-comparison",
   "metadata": {},
   "source": [
    "### User input required: Name of CSV file for report generated by this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-senator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supply name of CSV output file for tabular results generated by this notebook:\n",
    "#\n",
    "csv_output_fn = 'links_report_comp_scenario.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-prize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing link flow CSVs\n",
    "link_flow_dir = home_dir + 'out/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-picture",
   "metadata": {},
   "source": [
    "### User input required: Specify list of IDs of links for which to generate report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-battlefield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the IDs of the links for which to generate this report:\n",
    "#\n",
    "# Directory containing CSVs with the IDs of the sample links to use for this prototype\n",
    "sample_links_dir = reference_data_dir + 'sample_model_links/'\n",
    "#\n",
    "# CSV files with sample highway and transit links for this prototype.\n",
    "# (The latter will not be used in this prototype.)\n",
    "# NOTE: For the highway links, we use a superset of the links supplied by Drashti, \n",
    "#       encompassing more contiguous geometry of  the project area.\n",
    "highway_links_csv = sample_links_dir + 'highway_links_superset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-daniel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the above into a dataframe\n",
    "highway_links_df = pd.read_csv(highway_links_csv, delimiter=\",\")\n",
    "#\n",
    "# And convert the column with the link ID in each dataframe to a Python list\n",
    "#\n",
    "highway_links_list = highway_links_df['TC_Link_ID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the above into dataframes\n",
    "highway_links_df = pd.read_csv(highway_links_csv, delimiter=\",\")\n",
    "# transit_links_df = pd.read_csv(transit_links_csv, delimeter=\",\")\n",
    "#\n",
    "# And convert the column with the link ID in each dataframe to a Python list\n",
    "#\n",
    "highway_links_list = highway_links_df['TC_Link_ID'].tolist()\n",
    "# transit_links_list = transit_links_df['Route_ID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual link-flow CSV tables:\n",
    "# For each time period, there is a separate flow CSV for autos and for trucks.\n",
    "# To get the total volume for any given time period, 'Tot_Flow' columns these need to be summed.\n",
    "# However, the V/C and speed data for *both* autos and trucks are reported in the CSV for autos.\n",
    "# Clear?\n",
    "#\n",
    "am_flow_auto_fn = link_flow_dir + 'AM_MMA_LinkFlow.csv'\n",
    "am_flow_truck_fn = link_flow_dir + 'AM_MMA_LinkFlow_Trucks.csv'\n",
    "#\n",
    "md_flow_auto_fn = link_flow_dir + 'MD_MMA_LinkFlow.csv'\n",
    "md_flow_truck_fn = link_flow_dir + 'MD_MMA_LinkFlow_Trucks.csv'\n",
    "#\n",
    "pm_flow_auto_fn = link_flow_dir + 'PM_MMA_LinkFlow.csv'\n",
    "pm_flow_truck_fn = link_flow_dir + 'PM_MMA_LinkFlow_Trucks.csv'\n",
    "#\n",
    "nt_flow_auto_fn = link_flow_dir + 'NT_MMA_LinkFlow.csv'\n",
    "nt_flow_truck_fn = link_flow_dir + 'NT_MMA_LinkFlow_Trucks.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read each of the above CSV files containing flow data into a dataframe\n",
    "#\n",
    "temp_am_auto_df = pd.read_csv(am_flow_auto_fn, delimiter=',')\n",
    "temp_am_truck_df = pd.read_csv(am_flow_truck_fn, delimiter=',')\n",
    "#\n",
    "temp_md_auto_df = pd.read_csv(md_flow_auto_fn, delimiter=',')\n",
    "temp_md_truck_df = pd.read_csv(md_flow_truck_fn, delimiter=',')\n",
    "#\n",
    "temp_pm_auto_df = pd.read_csv(pm_flow_auto_fn, delimiter=',')\n",
    "temp_pm_truck_df = pd.read_csv(pm_flow_truck_fn, delimiter=',')\n",
    "#\n",
    "temp_nt_auto_df = pd.read_csv(nt_flow_auto_fn, delimiter=',')\n",
    "temp_nt_truck_df = pd.read_csv(nt_flow_truck_fn, delimiter=',') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the 8 temp \"flow\" dataframes to only include rows for the selected highway links\n",
    "#\n",
    "am_auto_df = temp_am_auto_df[temp_am_auto_df['ID1'].isin(highway_links_list)]\n",
    "am_truck_df = temp_am_truck_df[temp_am_truck_df['ID1'].isin(highway_links_list)]\n",
    "#\n",
    "md_auto_df = temp_md_auto_df[temp_md_auto_df['ID1'].isin(highway_links_list)]\n",
    "md_truck_df = temp_md_truck_df[temp_md_truck_df['ID1'].isin(highway_links_list)]\n",
    "#\n",
    "pm_auto_df = temp_pm_auto_df[temp_pm_auto_df['ID1'].isin(highway_links_list)]\n",
    "pm_truck_df = temp_pm_truck_df[temp_pm_truck_df['ID1'].isin(highway_links_list)]\n",
    "#\n",
    "nt_auto_df = temp_nt_auto_df[temp_nt_auto_df['ID1'].isin(highway_links_list)]\n",
    "nt_truck_df = temp_nt_truck_df[temp_nt_truck_df['ID1'].isin(highway_links_list)]\n",
    "#\n",
    "# NOTE: volume/capacity and speed data will be harvested from the \"auto\" dataframes subsequently.\n",
    "#       See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further filter the filetered \"flow\" datafames to only include the columns containing 'Tot_Flow' (and 'ID1')\n",
    "# \n",
    "am_auto_vol_df = am_auto_df[['ID1', 'Tot_Flow']]\n",
    "am_truck_vol_df = am_truck_df[['ID1', 'Tot_Flow']]\n",
    "#\n",
    "md_auto_vol_df = md_auto_df[['ID1', 'Tot_Flow']]\n",
    "md_truck_vol_df = md_truck_df[['ID1', 'Tot_Flow']]\n",
    "#\n",
    "pm_auto_vol_df = pm_auto_df[['ID1', 'Tot_Flow']]\n",
    "pm_truck_vol_df = pm_truck_df[['ID1', 'Tot_Flow']]\n",
    "#\n",
    "nt_auto_vol_df = nt_auto_df[['ID1', 'Tot_Flow']]\n",
    "nt_truck_vol_df = nt_truck_df[['ID1', 'Tot_Flow']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the 'Tot_Flow' column of each dataframe, appropriately\n",
    "#\n",
    "am_auto_vol_df = am_auto_vol_df.rename(columns={'Tot_Flow' : 'Tot_Flow_am_auto'})\n",
    "am_truck_vol_df = am_truck_vol_df.rename(columns={'Tot_Flow' : 'Tot_Flow_am_truck'})\n",
    "#\n",
    "md_auto_vol_df = md_auto_vol_df.rename(columns={'Tot_Flow' : 'Tot_Flow_md_auto'})\n",
    "md_truck_vol_df = md_truck_vol_df.rename(columns={'Tot_Flow' : 'Tot_Flow_md_truck'})\n",
    "#\n",
    "pm_auto_vol_df = pm_auto_vol_df.rename(columns={'Tot_Flow' : 'Tot_Flow_pm_auto'})\n",
    "pm_truck_vol_df = pm_truck_vol_df.rename(columns={'Tot_Flow' : 'Tot_Flow_pm_truck'})\n",
    "#\n",
    "nt_auto_vol_df = nt_auto_vol_df.rename(columns={'Tot_Flow' : 'Tot_Flow_nt_auto'})\n",
    "nt_truck_vol_df = nt_truck_vol_df.rename(columns={'Tot_Flow' : 'Tot_Flow_nt_truck'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index all the \"volume\" dataframes on \"ID1\", in preparation for joining\n",
    "#\n",
    "am_auto_vol_df.set_index(\"ID1\")\n",
    "am_truck_vol_df.set_index(\"ID1\")\n",
    "#\n",
    "md_auto_vol_df.set_index(\"ID1\")\n",
    "md_truck_vol_df.set_index(\"ID1\")\n",
    "#\n",
    "pm_auto_vol_df.set_index(\"ID1\")\n",
    "pm_truck_vol_df.set_index(\"ID1\")\n",
    "#\n",
    "nt_auto_vol_df.set_index(\"ID1\")\n",
    "nt_truck_vol_df.set_index(\"ID1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-offense",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the \"volume\" dataframes\n",
    "j1_df = am_auto_vol_df.join(am_truck_vol_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "#\n",
    "j1_df.set_index(\"ID1\")\n",
    "j2_df = j1_df.join(md_auto_vol_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "#\n",
    "j2_df.set_index(\"ID1\")\n",
    "j3_df = j2_df.join(md_truck_vol_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "#\n",
    "j3_df.set_index(\"ID1\")\n",
    "j4_df = j3_df.join(pm_auto_vol_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "#\n",
    "j4_df.set_index(\"ID1\")\n",
    "j5_df = j4_df.join(pm_truck_vol_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "#\n",
    "j5_df.set_index(\"ID1\")\n",
    "j6_df = j5_df.join(nt_auto_vol_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "#\n",
    "j6_df.set_index(\"ID1\")\n",
    "total_flow_join = j6_df.join(nt_truck_vol_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "#\n",
    "total_flow_join.set_index(\"ID1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total volume (auto + truck) for each time period, and for the entire day\n",
    "#\n",
    "total_flow_join['Tot_Flow_am'] = total_flow_join['Tot_Flow_am_auto'] + total_flow_join['Tot_Flow_am_truck']\n",
    "#\n",
    "total_flow_join['Tot_Flow_md'] = total_flow_join['Tot_Flow_md_auto'] + total_flow_join['Tot_Flow_md_truck']\n",
    "#\n",
    "total_flow_join['Tot_Flow_pm'] = total_flow_join['Tot_Flow_pm_auto'] + total_flow_join['Tot_Flow_pm_truck']\n",
    "#\n",
    "total_flow_join['Tot_Flow_nt'] = total_flow_join['Tot_Flow_nt_auto'] + total_flow_join['Tot_Flow_nt_truck']\n",
    "#\n",
    "total_flow_join['Tot_Flow_daily'] = total_flow_join['Tot_Flow_am'] + total_flow_join['Tot_Flow_md'] + \\\n",
    "                                    total_flow_join['Tot_Flow_pm'] + total_flow_join['Tot_Flow_nt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "total_flow_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_flow_join.set_index(\"ID1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harvest the speed and volume-to-capacity ratio data from the 4 \"auto\" dataframes, one for each time period. (See above.)\n",
    "# Note Python variable naming convention used here: \"svc\" == \"speed and volume/capacity\"\n",
    "#\n",
    "am_svc_df = am_auto_df[['ID1', 'AB_Speed', 'BA_Speed', 'AB_VOC', 'BA_VOC']]\n",
    "#\n",
    "md_svc_df = md_auto_df[['ID1', 'AB_Speed', 'BA_Speed', 'AB_VOC', 'BA_VOC']]\n",
    "#\n",
    "pm_svc_df = pm_auto_df[['ID1', 'AB_Speed', 'BA_Speed', 'AB_VOC', 'BA_VOC']]\n",
    "#\n",
    "nt_svc_df = nt_auto_df[['ID1', 'AB_Speed', 'BA_Speed', 'AB_VOC', 'BA_VOC']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-transcript",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "am_svc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns of these \"svc\" dataframes in preparation for joining them with the speed dataframe, computed above.\n",
    "#\n",
    "am_svc_df = am_svc_df.rename(columns={'AB_Speed' : 'AB_Speed_am', \n",
    "                                      'BA_Speed' : 'BA_Speed_am',\n",
    "                                      'AB_VOC'   : 'AB_VOC_am', \n",
    "                                      'BA_VOC'   : 'BA_VOC_am'})\n",
    "#\n",
    "md_svc_df = md_svc_df.rename(columns={'AB_Speed' : 'AB_Speed_md', \n",
    "                                      'BA_Speed' : 'BA_Speed_md',\n",
    "                                      'AB_VOC'   : 'AB_VOC_md', \n",
    "                                      'BA_VOC'   : 'BA_VOC_md'})\n",
    "#\n",
    "pm_svc_df = pm_svc_df.rename(columns={'AB_Speed' : 'AB_Speed_pm', \n",
    "                                      'BA_Speed' : 'BA_Speed_pm',\n",
    "                                      'AB_VOC'   : 'AB_VOC_pm', \n",
    "                                      'BA_VOC'   : 'BA_VOC_pm'})\n",
    "#\n",
    "nt_svc_df = nt_svc_df.rename(columns={'AB_Speed' : 'AB_Speed_nt', \n",
    "                                      'BA_Speed' : 'BA_Speed_nt',\n",
    "                                      'AB_VOC'   : 'AB_VOC_nt', \n",
    "                                      'BA_VOC'   : 'BA_VOC_nt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "nt_svc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per instructions from Marty on June 22, 2021:\n",
    "# For a given time period, calculate the MIN of the AB_Speed and BA_Speed, and the MAX of the AB_VOC and BA_VOC.\n",
    "# Basically, the idea is to flag the link direction with the most congestion.\n",
    "#\n",
    "am_svc_df['Speed_am'] = am_svc_df.apply(lambda x: min(x['AB_Speed_am'], x['BA_Speed_am']), axis=1)\n",
    "am_svc_df['VOC_am'] = am_svc_df.apply(lambda x: max(x['AB_VOC_am'], x['BA_VOC_am']), axis=1)\n",
    "#\n",
    "md_svc_df['Speed_md'] = md_svc_df.apply(lambda x: min(x['AB_Speed_md'], x['BA_Speed_md']), axis=1)\n",
    "md_svc_df['VOC_md'] = md_svc_df.apply(lambda x: max(x['AB_VOC_md'], x['BA_VOC_md']), axis=1)\n",
    "#\n",
    "pm_svc_df['Speed_pm'] = pm_svc_df.apply(lambda x: min(x['AB_Speed_pm'], x['BA_Speed_pm']), axis=1)\n",
    "pm_svc_df['VOC_pm'] = pm_svc_df.apply(lambda x: max(x['AB_VOC_pm'], x['BA_VOC_pm']), axis=1)\n",
    "#\n",
    "nt_svc_df['Speed_nt'] = nt_svc_df.apply(lambda x: min(x['AB_Speed_nt'], x['BA_Speed_nt']), axis=1)\n",
    "nt_svc_df['VOC_nt'] = nt_svc_df.apply(lambda x: max(x['AB_VOC_nt'], x['BA_VOC_nt']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Santiy check #1\n",
    "am_svc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Santiy check #2\n",
    "md_svc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Santiy check #3\n",
    "pm_svc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Santiy check #4\n",
    "nt_svc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index the \"svc\" dataframes in preparation for joining\n",
    "am_svc_df.set_index(\"ID1\")\n",
    "md_svc_df.set_index(\"ID1\")\n",
    "pm_svc_df.set_index(\"ID1\")\n",
    "nt_svc_df.set_index(\"ID1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-requirement",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Join the speed and volume/capacity data to the volume data collected above into a single dataframe.\n",
    "#\n",
    "j7_df = total_flow_join.join(am_svc_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "j7_df.set_index(\"ID1\")\n",
    "#\n",
    "j8_df = j7_df.join(md_svc_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "j8_df.set_index(\"ID1\")\n",
    "#\n",
    "j9_df = j8_df.join(pm_svc_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "j9_df.set_index(\"ID1\")\n",
    "#\n",
    "all_data_df = j9_df.join(nt_svc_df.set_index(\"ID1\"), on=\"ID1\")\n",
    "all_data_df.set_index(\"ID1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-desert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the dataframe as a CSV file\n",
    "fq_output_fn = sandbox_dir + csv_output_fn\n",
    "all_data_df.to_csv(fq_output_fn, sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-magnitude",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:modx_proto1] *",
   "language": "python",
   "name": "conda-env-modx_proto1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
